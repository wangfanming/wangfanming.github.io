<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.wfanming.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":10,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前言每次当我们写完spark代码后，通过spark-submit命令提交执行。但是这个过程中发生了哪些事情却不甚明了。今天就来看看这个过程中发生了什么。 一切从下边这段提交命令开始： spark-submit  --class org.apache.spark.examples.SparkPi \    --master yarn \    --conf spark.eventLog.dir&#x3D;hd">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark on Yarn框架初始化">
<meta property="og:url" content="https://www.wfanming.top/2024/03/17/Spark%E6%A1%86%E6%9E%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B/index.html">
<meta property="og:site_name" content="Duncan&#39;s Blog">
<meta property="og:description" content="前言每次当我们写完spark代码后，通过spark-submit命令提交执行。但是这个过程中发生了哪些事情却不甚明了。今天就来看看这个过程中发生了什么。 一切从下边这段提交命令开始： spark-submit  --class org.apache.spark.examples.SparkPi \    --master yarn \    --conf spark.eventLog.dir&#x3D;hd">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-03-17T13:14:22.000Z">
<meta property="article:modified_time" content="2024-05-17T17:35:12.224Z">
<meta property="article:author" content="Wang Fanming">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="yarn">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www.wfanming.top/2024/03/17/Spark%E6%A1%86%E6%9E%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark on Yarn框架初始化 | Duncan's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Duncan's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">何以解忧，唯有学习</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/wangfanming" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wfanming.top/2024/03/17/Spark%E6%A1%86%E6%9E%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Fanming">
      <meta itemprop="description" content="吾生也有涯，而知也无涯，以有涯随无涯，殆己！已而为知者，殆而已矣。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Duncan's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark on Yarn框架初始化
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-17 21:14:22" itemprop="dateCreated datePublished" datetime="2024-03-17T21:14:22+08:00">2024-03-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-05-18 01:35:12" itemprop="dateModified" datetime="2024-05-18T01:35:12+08:00">2024-05-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>30 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>每次当我们写完spark代码后，通过spark-submit命令提交执行。但是这个过程中发生了哪些事情却不甚明了。今天就来看看这个过程中发生了什么。</p>
<p>一切从下边这段提交命令开始：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">spark-submit  --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">\</span></span></span><br><span class="line">    --master yarn \</span><br><span class="line">    --conf spark.eventLog.dir=hdfs:<span class="comment">//dbmtimehadoop/tmp/spark2 \</span></span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory <span class="number">4</span>g \</span><br><span class="line">    --executor-memory <span class="number">2</span>g \</span><br><span class="line">    --executor-cores <span class="number">1</span> \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    $<span class="type">SPARK_HOME</span>/examples/jars/spark-examples*.jar \</span><br><span class="line">    <span class="number">10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h2><p>通过命令行我们可以看出来任务是通过<code>spark-submit</code>这个脚本进行提交的。所以我们从这个脚本入口进行分析。</p>
<ol>
<li><p>位于spark安装包的bin目录下的spark-submit脚本：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">source</span> <span class="string">&quot;<span class="subst">$(dirname <span class="string">&quot;<span class="variable">$0</span>&quot;</span>)</span>&quot;</span>/find-spark-home</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># disable randomized hash for string in Python 3.3+</span></span><br><span class="line"><span class="built_in">export</span> PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span>/bin/spark-class org.apache.spark.deploy.SparkSubmit <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br></pre></td></tr></table></figure>

<p>内容很简单，就是首先查找到SPARK_HOME的目录所在，继而使用<code>spark-class</code>脚本启动SparkSubmit，并将我们所有从参数都传递给了<code>org.apache.spark.deploy.SparkSubmit</code>。</p>
</li>
<li><p>位于spark安装包的bin目录下的spark-class脚本：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">source</span> <span class="string">&quot;<span class="subst">$(dirname <span class="string">&quot;<span class="variable">$0</span>&quot;</span>)</span>&quot;</span>/find-spark-home</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">. <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>&quot;</span>/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find the java binary</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$&#123;JAVA_HOME&#125;</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  RUNNER=<span class="string">&quot;<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin/java&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="keyword">if</span> [ <span class="string">&quot;<span class="subst">$(command -v java)</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    RUNNER=<span class="string">&quot;java&quot;</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;JAVA_HOME is not set&quot;</span> &gt;&amp;2</span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find Spark jars.</span></span><br><span class="line"><span class="keyword">if</span> [ -d <span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>/jars&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  SPARK_JARS_DIR=<span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>/jars&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  SPARK_JARS_DIR=<span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>/assembly/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/jars&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="string">&quot;<span class="variable">$SPARK_JARS_DIR</span>&quot;</span> ] &amp;&amp; [ -z <span class="string">&quot;$SPARK_TESTING<span class="variable">$SPARK_SQL_TESTING</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;Failed to find Spark jars directory (<span class="variable">$SPARK_JARS_DIR</span>).&quot;</span> 1&gt;&amp;2</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;You need to build Spark with the target \&quot;package\&quot; before running this program.&quot;</span> 1&gt;&amp;2</span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  LAUNCH_CLASSPATH=<span class="string">&quot;<span class="variable">$SPARK_JARS_DIR</span>/*&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the launcher build dir to the classpath if requested.</span></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$SPARK_PREPEND_CLASSES</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  LAUNCH_CLASSPATH=<span class="string">&quot;<span class="variable">$&#123;SPARK_HOME&#125;</span>/launcher/target/scala-<span class="variable">$SPARK_SCALA_VERSION</span>/classes:<span class="variable">$LAUNCH_CLASSPATH</span>&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># For tests</span></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="string">&quot;<span class="variable">$SPARK_TESTING</span>&quot;</span> ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">unset</span> YARN_CONF_DIR</span><br><span class="line">  <span class="built_in">unset</span> HADOOP_CONF_DIR</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The launcher library will print arguments separated by a NULL character, to allow arguments with</span></span><br><span class="line"><span class="comment"># characters that would be otherwise interpreted by the shell. Read that in a while loop, populating</span></span><br><span class="line"><span class="comment"># an array that will be used to exec the final command.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The exit code of the launcher is appended to the output, so the parent shell removes it from the</span></span><br><span class="line"><span class="comment"># command array and checks the value to see if the launcher succeeded.</span></span><br><span class="line"><span class="function"><span class="title">build_command</span></span>() &#123;</span><br><span class="line">  <span class="string">&quot;<span class="variable">$RUNNER</span>&quot;</span> -Xmx128m -<span class="built_in">cp</span> <span class="string">&quot;<span class="variable">$LAUNCH_CLASSPATH</span>&quot;</span> org.apache.spark.launcher.Main <span class="string">&quot;<span class="variable">$@</span>&quot;</span></span><br><span class="line">  <span class="built_in">printf</span> <span class="string">&quot;%d\0&quot;</span> $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn off posix mode since it does not allow process substitution</span></span><br><span class="line"><span class="built_in">set</span> +o posix</span><br><span class="line">CMD=()</span><br><span class="line"><span class="comment"># 在使用spark-submit启动任务的过程中，将org.apache.spark.launcher.Main的main方法的输出作为启动命令，用来启动org.apache.spark.deploy.SparkSubmit</span></span><br><span class="line"><span class="keyword">while</span> IFS= <span class="built_in">read</span> -d <span class="string">&#x27;&#x27;</span> -r ARG; <span class="keyword">do</span></span><br><span class="line">  CMD+=(<span class="string">&quot;<span class="variable">$ARG</span>&quot;</span>)</span><br><span class="line"><span class="keyword">done</span> &lt; &lt;(build_command <span class="string">&quot;<span class="variable">$@</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">COUNT=<span class="variable">$&#123;#CMD[@]&#125;</span></span><br><span class="line">LAST=$((COUNT - <span class="number">1</span>))</span><br><span class="line">LAUNCHER_EXIT_CODE=<span class="variable">$&#123;CMD[$LAST]&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes</span></span><br><span class="line"><span class="comment"># the code that parses the output of the launcher to get confused. In those cases, check if the</span></span><br><span class="line"><span class="comment"># exit code is an integer, and if it&#x27;s not, handle it as a special error case.</span></span><br><span class="line"><span class="keyword">if</span> ! [[ <span class="variable">$LAUNCHER_EXIT_CODE</span> =~ ^[0-9]+$ ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span> | <span class="built_in">head</span> -n-1 1&gt;&amp;2</span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$LAUNCHER_EXIT_CODE</span> != 0 ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">exit</span> <span class="variable">$LAUNCHER_EXIT_CODE</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">CMD=(<span class="string">&quot;<span class="variable">$&#123;CMD[@]:0:$LAST&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">exec</span> <span class="string">&quot;<span class="variable">$&#123;CMD[@]&#125;</span>&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上述内容，简单来说就是首先检查一下SPARK_HOME、JAVA_HOME、LAUNCH_CLASSPATH等这些环境变量是否配置，没有配置就进行使用相关脚本进行搜索，然后调用org.apache.spark.launcher.Main的main方法，并将main的输出作为最后启动org.apache.spark.deploy.SparkSubmit的参数的一部分。</p>
</li>
<li><p>org.apache.spark.launcher.Main</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">public static void main(String[] argsArray) throws Exception &#123;</span><br><span class="line">    checkArgument(argsArray.length &gt; 0, &quot;Not enough arguments: missing class name.&quot;);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; args = new ArrayList&lt;&gt;(Arrays.asList(argsArray));</span><br><span class="line">    String className = args.remove(0);</span><br><span class="line"></span><br><span class="line">    boolean printLaunchCommand = !isEmpty(System.getenv(&quot;SPARK_PRINT_LAUNCH_COMMAND&quot;));</span><br><span class="line">    Map&lt;String, String&gt; env = new HashMap&lt;&gt;();</span><br><span class="line">    List&lt;String&gt; cmd;</span><br><span class="line">    if (className.equals(&quot;org.apache.spark.deploy.SparkSubmit&quot;)) &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        AbstractCommandBuilder builder = new SparkSubmitCommandBuilder(args);</span><br><span class="line">        // 根据参数配置构建spark-submit命令</span><br><span class="line">        cmd = buildCommand(builder, env, printLaunchCommand);</span><br><span class="line">      ...</span><br><span class="line">    if (isWindows()) &#123;</span><br><span class="line">      System.out.println(prepareWindowsCommand(cmd, env));</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // In bash, use NULL as the arg separator since it cannot be used in an argument.</span><br><span class="line">      List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env);</span><br><span class="line">      // 打印spark-submit命令,将输出作为参数传递给spark-class脚本</span><br><span class="line">      for (String c : bashCmd) &#123;</span><br><span class="line">        System.out.print(c);</span><br><span class="line">        System.out.print(&#x27;\0&#x27;);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>然后org.apache.spark.deploy.SparkSubmit的main方法启动</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> submit = <span class="keyword">new</span> <span class="type">SparkSubmit</span>() &#123;</span><br><span class="line">     ...</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   submit.doSubmit(args)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    ...</span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// 在SparkSubmitArguments初始化时的默认值</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">PRINT_VERSION</span> =&gt; printVersion()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>, uninitLog: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</span><br><span class="line">        ...</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 按照启动参数，启动Saprk框架</span></span><br><span class="line">        runMain(args, uninitLog)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">    <span class="comment">// standalone模式下的启动过程</span></span><br><span class="line">    <span class="keyword">if</span> (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Running Spark using the REST application submission protocol.&quot;</span>)</span><br><span class="line">        doRunMain()</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="comment">// Fail over to use the legacy submission gateway</span></span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">SubmitRestConnectionException</span> =&gt;</span><br><span class="line">          logWarning(<span class="string">s&quot;Master endpoint <span class="subst">$&#123;args.master&#125;</span> was not a REST server. &quot;</span> +</span><br><span class="line">            <span class="string">&quot;Falling back to legacy submission gateway instead.&quot;</span>)</span><br><span class="line">          args.useRest = <span class="literal">false</span></span><br><span class="line">          submit(args, <span class="literal">false</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// In all other modes, just run the main class as prepared</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 启动用户主类</span></span><br><span class="line">      doRunMain()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(args: <span class="type">SparkSubmitArguments</span>, uninitLog: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 解析参数，下载资源文件，设置环境变量</span></span><br><span class="line">    <span class="keyword">val</span> (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">val</span> loader =</span><br><span class="line">      <span class="keyword">if</span> (sparkConf.get(<span class="type">DRIVER_USER_CLASS_PATH_FIRST</span>)) &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</span><br><span class="line">          <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</span><br><span class="line">          <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</span><br><span class="line">      addJarToClasspath(jar, loader)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> mainClass: <span class="type">Class</span>[_] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// yarn集群模式下，mainclass:org.apache.spark.deploy.yarn.YarnClusterApplication</span></span><br><span class="line">      mainClass = <span class="type">Utils</span>.classForName(childMainClass)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> app: <span class="type">SparkApplication</span> = <span class="keyword">if</span> (classOf[<span class="type">SparkApplication</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">      mainClass.newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// SPARK-4170</span></span><br><span class="line">      <span class="keyword">if</span> (classOf[scala.<span class="type">App</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">        logWarning(<span class="string">&quot;Subclasses of scala.App may not work correctly. Use a main() method instead.&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">JavaMainApplication</span>(mainClass)</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// yarn-cluster模式下，启动org.apache.spark.deploy.yarn.YarnClusterApplication#start</span></span><br><span class="line">      app.start(childArgs.toArray, sparkConf)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> findCause(t)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>YarnClusterApplication是位于源码目录resource-managers&#x2F;yarn&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;deploy&#x2F;yarn&#x2F;Client.scala的内部类，在程序运行时会被加载至classpath。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">YarnClusterApplication</span> <span class="keyword">extends</span> <span class="title">SparkApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], conf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// SparkSubmit would use yarn cache to distribute files &amp; jars in yarn mode,</span></span><br><span class="line">    <span class="comment">// so remove them from sparkConf here for yarn mode.</span></span><br><span class="line">    conf.remove(<span class="string">&quot;spark.jars&quot;</span>)</span><br><span class="line">    conf.remove(<span class="string">&quot;spark.files&quot;</span>)</span><br><span class="line">	<span class="comment">// 初始化Client对象，解析AM、Executor的参数,然后提交应用</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf).run()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;	</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 向Yarn提交应用,启动AM,在AM启动后，由其向NodeManager申请容器启动Executor</span></span><br><span class="line">    <span class="keyword">this</span>.appId = submitApplication()</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(): <span class="type">ApplicationId</span> = &#123;</span><br><span class="line">    <span class="keyword">var</span> appId: <span class="type">ApplicationId</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      launcherBackend.connect()</span><br><span class="line">      yarnClient.init(hadoopConf)</span><br><span class="line">      yarnClient.start()</span><br><span class="line"></span><br><span class="line">      logInfo(<span class="string">&quot;Requesting a new application from cluster with %d NodeManagers&quot;</span></span><br><span class="line">        .format(yarnClient.getYarnClusterMetrics.getNumNodeManagers))</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Get a new application from our RM</span></span><br><span class="line">      <span class="keyword">val</span> newApp = yarnClient.createApplication()</span><br><span class="line">      <span class="keyword">val</span> newAppResponse = newApp.getNewApplicationResponse()</span><br><span class="line">      appId = newAppResponse.getApplicationId()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">new</span> <span class="type">CallerContext</span>(<span class="string">&quot;CLIENT&quot;</span>, sparkConf.get(<span class="type">APP_CALLER_CONTEXT</span>),</span><br><span class="line">        <span class="type">Option</span>(appId.toString)).setCurrentContext()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Verify whether the cluster has enough resources for our AM</span></span><br><span class="line">      <span class="comment">// 检查RM资源是否可以满足AM的需求</span></span><br><span class="line">      verifyClusterResources(newAppResponse)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Set up the appropriate contexts to launch our AM</span></span><br><span class="line">      <span class="comment">// 配置AM的环境变量，上传资源文件到获得方式，拼凑AM的启动命令</span></span><br><span class="line">      <span class="keyword">val</span> containerContext = createContainerLaunchContext(newAppResponse)</span><br><span class="line">      <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Finally, submit and monitor the application</span></span><br><span class="line">      logInfo(<span class="string">s&quot;Submitting application <span class="subst">$appId</span> to ResourceManager&quot;</span>)</span><br><span class="line">      <span class="comment">// 启动AM:ApplicationMaster,AM类的main方法会被调用,继而开始进行Executor的申请和启动,在cluster模式下，用户的Driver线程会在AM的main方法中启动</span></span><br><span class="line">      yarnClient.submitApplication(appContext)</span><br><span class="line">      launcherBackend.setAppId(appId.toString)</span><br><span class="line">      reportLauncherState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">SUBMITTED</span>)</span><br><span class="line"></span><br><span class="line">      appId</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">if</span> (appId != <span class="literal">null</span>) &#123;</span><br><span class="line">          cleanupStagingDir(appId)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>到这里，ApplicationMaster类的main方法开始启动。在Yarn-cluster模式下，接下来会开始启动Executor和用户编写的应用程序。</li>
</ul>
</li>
<li><p>启动Executor和用户编写的应用程序</p>
<p>org.apache.spark.deploy.yarn.ApplicationMaster#main：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">SignalUtils</span>.registerLogger(log)</span><br><span class="line">    <span class="keyword">val</span> amArgs = <span class="keyword">new</span> <span class="type">ApplicationMasterArguments</span>(args)</span><br><span class="line">    master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs)</span><br><span class="line">    <span class="type">System</span>.exit(master.run())</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    doAsUser &#123;</span><br><span class="line">      runImpl()</span><br><span class="line">    &#125;</span><br><span class="line">    exitCode</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runImpl</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> appAttemptId = client.getAttemptId()</span><br><span class="line">      ...</span><br><span class="line">      <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">        <span class="comment">// 启动用户类线程</span></span><br><span class="line">        runDriver()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        runExecutorLauncher()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    addAmIpFilter(<span class="type">None</span>)</span><br><span class="line">    <span class="comment">// 在用户类线程中启动SparkContext，启动用户编写的Spark应用</span></span><br><span class="line">    userClassThread = startUserApplication()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This a bit hacky, but we need to wait until the spark.driver.port property has</span></span><br><span class="line">    <span class="comment">// been set by the Thread executing the user class.</span></span><br><span class="line">    logInfo(<span class="string">&quot;Waiting for spark context initialization...&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> totalWaitTime = sparkConf.get(<span class="type">AM_MAX_WAIT_TIME</span>)</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 等待SparkContext初始化，由sparkContextPromise来保证</span></span><br><span class="line">      <span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</span><br><span class="line">        <span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</span><br><span class="line">      <span class="keyword">if</span> (sc != <span class="literal">null</span>) &#123;</span><br><span class="line">        rpcEnv = sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> userConf = sc.getConf</span><br><span class="line">        <span class="keyword">val</span> host = userConf.get(<span class="string">&quot;spark.driver.host&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> port = userConf.get(<span class="string">&quot;spark.driver.port&quot;</span>).toInt</span><br><span class="line">        registerAM(host, port, userConf, sc.ui.map(_.webUrl))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> driverRef = rpcEnv.setupEndpointRef(</span><br><span class="line">          <span class="type">RpcAddress</span>(host, port),</span><br><span class="line">          <span class="type">YarnSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">        <span class="comment">// 创建资源分配器，启动Executor</span></span><br><span class="line">        createAllocator(driverRef, userConf)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Sanity check; should never happen in normal operation, since sc should only be null</span></span><br><span class="line">        <span class="comment">// if the user app did not create a SparkContext.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;User did not initialize spark context!&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Executor已经启动，恢复用户类线程</span></span><br><span class="line">      resumeDriver()</span><br><span class="line">      <span class="comment">// 等待用户类线程结束</span></span><br><span class="line">      userClassThread.join()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">SparkException</span> <span class="keyword">if</span> e.getCause().isInstanceOf[<span class="type">TimeoutException</span>] =&gt;</span><br><span class="line">        logError(</span><br><span class="line">          <span class="string">s&quot;SparkContext did not initialize after waiting for <span class="subst">$totalWaitTime</span> ms. &quot;</span> +</span><br><span class="line">           <span class="string">&quot;Please check earlier log output for errors. Failing the application.&quot;</span>)</span><br><span class="line">        finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">          <span class="type">ApplicationMaster</span>.<span class="type">EXIT_SC_NOT_INITED</span>,</span><br><span class="line">          <span class="string">&quot;Timed out waiting for SparkContext.&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      resumeDriver()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAllocator</span></span>(driverRef: <span class="type">RpcEndpointRef</span>, _sparkConf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// YarnRMClient</span></span><br><span class="line">  allocator = client.createAllocator(</span><br><span class="line">    yarnConf,</span><br><span class="line">    _sparkConf,</span><br><span class="line">    driverUrl,</span><br><span class="line">    driverRef,</span><br><span class="line">    securityMgr,</span><br><span class="line">    localResources)</span><br><span class="line">   </span><br><span class="line">  credentialRenewer.foreach(_.setDriverRef(driverRef))</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Initialize the AM endpoint *after* the allocator has been initialized. This ensures</span></span><br><span class="line">  <span class="comment">// that when the driver sends an initial executor request (e.g. after an AM restart),</span></span><br><span class="line">  <span class="comment">// the allocator is ready to service requests.</span></span><br><span class="line">  rpcEnv.setupEndpoint(<span class="string">&quot;YarnAM&quot;</span>, <span class="keyword">new</span> <span class="type">AMEndpoint</span>(rpcEnv, driverRef))</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// 在分配到的容器内启动Executor进程</span></span><br><span class="line">  allocator.allocateResources()</span><br><span class="line">  <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">&quot;applicationMaster&quot;</span>, sparkConf, securityMgr)</span><br><span class="line">  <span class="keyword">val</span> prefix = _sparkConf.get(<span class="type">YARN_METRICS_NAMESPACE</span>).getOrElse(appId)</span><br><span class="line">  ms.registerSource(<span class="keyword">new</span> <span class="type">ApplicationMasterSource</span>(prefix, allocator))</span><br><span class="line">  <span class="comment">// do not register static sources in this case as per SPARK-25277</span></span><br><span class="line">  ms.start(<span class="literal">false</span>)</span><br><span class="line">  metricsSystem = <span class="type">Some</span>(ms)</span><br><span class="line">  reporterThread = launchReporterThread()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">allocateResources</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">   ...</span><br><span class="line">	<span class="comment">// 处理已分配到的容器</span></span><br><span class="line">     handleAllocatedContainers(allocatedContainers.asScala)</span><br><span class="line">   &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleAllocatedContainers</span></span>(allocatedContainers: <span class="type">Seq</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 在容器中启动executor</span></span><br><span class="line">  runAllocatedContainers(containersToUse)</span><br><span class="line">   </span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (runningExecutors.size() &lt; targetNumExecutors) &#123;</span><br><span class="line">        numExecutorsStarting.incrementAndGet()</span><br><span class="line">        <span class="keyword">if</span> (launchContainers) &#123;</span><br><span class="line">          <span class="comment">// 使用线程池启动executor</span></span><br><span class="line">          launcherPool.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">                  <span class="type">Some</span>(container),</span><br><span class="line">                  conf,</span><br><span class="line">                  sparkConf,</span><br><span class="line">                  driverUrl,</span><br><span class="line">                  executorId,</span><br><span class="line">                  executorHostname,</span><br><span class="line">                  executorMemory,</span><br><span class="line">                  executorCores,</span><br><span class="line">                  appAttemptId.getApplicationId.toString,</span><br><span class="line">                  securityMgr,</span><br><span class="line">                  localResources</span><br><span class="line">                ).run()</span><br><span class="line">                updateInternalState()</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          ...</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来开始Executor的启动过程</p>
<p>org.apache.spark.deploy.yarn.ExecutorRunnable#run</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  logDebug(<span class="string">&quot;Starting Executor Container&quot;</span>)</span><br><span class="line">  nmClient = <span class="type">NMClient</span>.createNMClient()</span><br><span class="line">  nmClient.init(conf)</span><br><span class="line">  nmClient.start()</span><br><span class="line">  startContainer()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startContainer</span></span>(): java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ByteBuffer</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> ctx = <span class="type">Records</span>.newRecord(classOf[<span class="type">ContainerLaunchContext</span>])</span><br><span class="line">    .asInstanceOf[<span class="type">ContainerLaunchContext</span>]</span><br><span class="line">  <span class="keyword">val</span> env = prepareEnvironment().asJava</span><br><span class="line">   </span><br><span class="line">  ctx.setLocalResources(localResources.asJava)</span><br><span class="line">  ctx.setEnvironment(env)</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">val</span> credentials = <span class="type">UserGroupInformation</span>.getCurrentUser().getCredentials()</span><br><span class="line">  <span class="keyword">val</span> dob = <span class="keyword">new</span> <span class="type">DataOutputBuffer</span>()</span><br><span class="line">  credentials.writeTokenStorageToStream(dob)</span><br><span class="line">  ctx.setTokens(<span class="type">ByteBuffer</span>.wrap(dob.getData()))</span><br><span class="line">  <span class="comment">// 启动Executor的主进程CoarseGrainedExecutorBackend</span></span><br><span class="line">  <span class="keyword">val</span> commands = prepareCommand()</span><br><span class="line">   </span><br><span class="line">  ctx.setCommands(commands.asJava)</span><br><span class="line">  ctx.setApplicationACLs(</span><br><span class="line">    <span class="type">YarnSparkHadoopUtil</span>.getApplicationAclsForYarn(securityMgr).asJava)</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// If external shuffle service is enabled, register with the Yarn shuffle service already</span></span><br><span class="line">  <span class="comment">// started on the NodeManager and, if authentication is enabled, provide it with our secret</span></span><br><span class="line">  <span class="comment">// key for fetching shuffle files later</span></span><br><span class="line">  <span class="keyword">if</span> (sparkConf.get(<span class="type">SHUFFLE_SERVICE_ENABLED</span>)) &#123;</span><br><span class="line">    <span class="keyword">val</span> secretString = securityMgr.getSecretKey()</span><br><span class="line">    <span class="keyword">val</span> secretBytes =</span><br><span class="line">      <span class="keyword">if</span> (secretString != <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="comment">// This conversion must match how the YarnShuffleService decodes our secret</span></span><br><span class="line">        <span class="type">JavaUtils</span>.stringToBytes(secretString)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Authentication is not enabled, so just provide dummy metadata</span></span><br><span class="line">        <span class="type">ByteBuffer</span>.allocate(<span class="number">0</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ctx.setServiceData(<span class="type">Collections</span>.singletonMap(<span class="string">&quot;spark_shuffle&quot;</span>, secretBytes))</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Send the start request to the ContainerManager</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 通过NodeManager客户端，启动具体的容器进程</span></span><br><span class="line">    nmClient.startContainer(container.get, ctx)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Exception while starting container <span class="subst">$&#123;container.get.getId&#125;</span>&quot;</span> +</span><br><span class="line">        <span class="string">s&quot; on host <span class="subst">$hostname</span>&quot;</span>, ex)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareCommand</span></span>(): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">  <span class="comment">// Extra options for the JVM</span></span><br><span class="line">  <span class="keyword">val</span> javaOpts = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Set the JVM memory</span></span><br><span class="line">  <span class="keyword">val</span> executorMemoryString = executorMemory + <span class="string">&quot;m&quot;</span></span><br><span class="line">  javaOpts += <span class="string">&quot;-Xmx&quot;</span> + executorMemoryString</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Set extra Java options for the executor, if defined</span></span><br><span class="line">  sparkConf.get(<span class="type">EXECUTOR_JAVA_OPTIONS</span>).foreach &#123; opts =&gt;</span><br><span class="line">    <span class="keyword">val</span> subsOpt = <span class="type">Utils</span>.substituteAppNExecIds(opts, appId, executorId)</span><br><span class="line">    javaOpts ++= <span class="type">Utils</span>.splitCommandString(subsOpt).map(<span class="type">YarnSparkHadoopUtil</span>.escapeForShell)</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Set the library path through a command prefix to append to the existing value of the</span></span><br><span class="line">  <span class="comment">// env variable.</span></span><br><span class="line">  <span class="keyword">val</span> prefixEnv = sparkConf.get(<span class="type">EXECUTOR_LIBRARY_PATH</span>).map &#123; libPath =&gt;</span><br><span class="line">    <span class="type">Client</span>.createLibraryPathPrefix(libPath, sparkConf)</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  javaOpts += <span class="string">&quot;-Djava.io.tmpdir=&quot;</span> +</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Path</span>(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_CONTAINER_TEMP_DIR</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Certain configs need to be passed here because they are needed before the Executor</span></span><br><span class="line">  <span class="comment">// registers with the Scheduler and transfers the spark configs. Since the Executor backend</span></span><br><span class="line">  <span class="comment">// uses RPC to connect to the scheduler, the RPC settings are needed as well as the</span></span><br><span class="line">  <span class="comment">// authentication settings.</span></span><br><span class="line">  sparkConf.getAll</span><br><span class="line">    .filter &#123; <span class="keyword">case</span> (k, v) =&gt; <span class="type">SparkConf</span>.isExecutorStartupConf(k) &#125;</span><br><span class="line">    .foreach &#123; <span class="keyword">case</span> (k, v) =&gt; javaOpts += <span class="type">YarnSparkHadoopUtil</span>.escapeForShell(<span class="string">s&quot;-D<span class="subst">$k</span>=<span class="subst">$v</span>&quot;</span>) &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// Commenting it out for now - so that people can refer to the properties if required. Remove</span></span><br><span class="line">  <span class="comment">// it once cpuset version is pushed out.</span></span><br><span class="line">  <span class="comment">// The context is, default gc for server class machines end up using all cores to do gc - hence</span></span><br><span class="line">  <span class="comment">// if there are multiple containers in same node, spark gc effects all other containers</span></span><br><span class="line">  <span class="comment">// performance (which can also be other spark containers)</span></span><br><span class="line">  <span class="comment">// Instead of using this, rely on cpusets by YARN to enforce spark behaves &#x27;properly&#x27; in</span></span><br><span class="line">  <span class="comment">// multi-tenant environments. Not sure how default java gc behaves if it is limited to subset</span></span><br><span class="line">  <span class="comment">// of cores on a node.</span></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">      else &#123;</span></span><br><span class="line"><span class="comment">        // If no java_opts specified, default to using -XX:+CMSIncrementalMode</span></span><br><span class="line"><span class="comment">        // It might be possible that other modes/config is being done in</span></span><br><span class="line"><span class="comment">        // spark.executor.extraJavaOptions, so we don&#x27;t want to mess with it.</span></span><br><span class="line"><span class="comment">        // In our expts, using (default) throughput collector has severe perf ramifications in</span></span><br><span class="line"><span class="comment">        // multi-tenant machines</span></span><br><span class="line"><span class="comment">        // The options are based on</span></span><br><span class="line"><span class="comment">        // http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html#0.0.0.%20When%20to%20Use</span></span><br><span class="line"><span class="comment">        // %20the%20Concurrent%20Low%20Pause%20Collector|outline</span></span><br><span class="line"><span class="comment">        javaOpts += &quot;-XX:+UseConcMarkSweepGC&quot;</span></span><br><span class="line"><span class="comment">        javaOpts += &quot;-XX:+CMSIncrementalMode&quot;</span></span><br><span class="line"><span class="comment">        javaOpts += &quot;-XX:+CMSIncrementalPacing&quot;</span></span><br><span class="line"><span class="comment">        javaOpts += &quot;-XX:CMSIncrementalDutyCycleMin=0&quot;</span></span><br><span class="line"><span class="comment">        javaOpts += &quot;-XX:CMSIncrementalDutyCycle=10&quot;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">   </span><br><span class="line">  <span class="comment">// For log4j configuration to reference</span></span><br><span class="line">  javaOpts += (<span class="string">&quot;-Dspark.yarn.app.container.log.dir=&quot;</span> + <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">val</span> userClassPath = <span class="type">Client</span>.getUserClasspath(sparkConf).flatMap &#123; uri =&gt;</span><br><span class="line">    <span class="keyword">val</span> absPath =</span><br><span class="line">      <span class="keyword">if</span> (<span class="keyword">new</span> <span class="type">File</span>(uri.getPath()).isAbsolute()) &#123;</span><br><span class="line">        <span class="type">Client</span>.getClusterPath(sparkConf, uri.getPath())</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Client</span>.buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$(), uri.getPath())</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;--user-class-path&quot;</span>, <span class="string">&quot;file:&quot;</span> + absPath)</span><br><span class="line">  &#125;.toSeq</span><br><span class="line">   </span><br><span class="line">  <span class="type">YarnSparkHadoopUtil</span>.addOutOfMemoryErrorArgument(javaOpts)</span><br><span class="line">  <span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line">    <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">    javaOpts ++</span><br><span class="line">  <span class="comment">// Executor进程的主类：CoarseGrainedExecutorBackend</span></span><br><span class="line">    <span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;</span>,</span><br><span class="line">      <span class="string">&quot;--driver-url&quot;</span>, masterAddress,</span><br><span class="line">      <span class="string">&quot;--executor-id&quot;</span>, executorId,</span><br><span class="line">      <span class="string">&quot;--hostname&quot;</span>, hostname,</span><br><span class="line">      <span class="string">&quot;--cores&quot;</span>, executorCores.toString,</span><br><span class="line">      <span class="string">&quot;--app-id&quot;</span>, appId) ++</span><br><span class="line">    userClassPath ++</span><br><span class="line">    <span class="type">Seq</span>(</span><br><span class="line">      <span class="string">s&quot;1&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stdout&quot;</span>,</span><br><span class="line">      <span class="string">s&quot;2&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stderr&quot;</span>)</span><br><span class="line">   </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> it would be nicer to just make sure there are no null commands here</span></span><br><span class="line">  commands.map(s =&gt; <span class="keyword">if</span> (s == <span class="literal">null</span>) <span class="string">&quot;null&quot;</span> <span class="keyword">else</span> s).toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来进行CoarseGrainedExecutorBackend的初始化过程，这里就涉及到了消息系统的初始化过程。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">CoarseGrainedExecutorBackend</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    ...</span><br><span class="line">    run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath)</span><br><span class="line">    <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(</span><br><span class="line">      driverUrl: <span class="type">String</span>,</span><br><span class="line">      executorId: <span class="type">String</span>,</span><br><span class="line">      hostname: <span class="type">String</span>,</span><br><span class="line">      cores: <span class="type">Int</span>,</span><br><span class="line">      appId: <span class="type">String</span>,</span><br><span class="line">      workerUrl: <span class="type">Option</span>[<span class="type">String</span>],</span><br><span class="line">      userClassPath: <span class="type">Seq</span>[<span class="type">URL</span>]) &#123;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">   <span class="comment">// 这里初始化了一个基于Netty的的消息系统，并且在初始化时会创建一个线程池，用于处理消息，且发送类一个OnStart事件用来启动消息处理</span></span><br><span class="line">      <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.createExecutorEnv(</span><br><span class="line">        driverConf, executorId, hostname, cores, cfg.ioEncryptionKey, isLocal = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">      env.rpcEnv.setupEndpoint(<span class="string">&quot;Executor&quot;</span>, <span class="keyword">new</span> <span class="type">CoarseGrainedExecutorBackend</span>(</span><br><span class="line">        env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env))</span><br><span class="line">      workerUrl.foreach &#123; url =&gt;</span><br><span class="line">        env.rpcEnv.setupEndpoint(<span class="string">&quot;WorkerWatcher&quot;</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(env.rpcEnv, url))</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// 启动消息处理线程</span></span><br><span class="line">      env.rpcEnv.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下为消息处理线程相关信息，它们主要来自于org.apache.spark.rpc.netty.Dispatcher</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    dispatcher.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    threadpool.awaitTermination(<span class="type">Long</span>.<span class="type">MaxValue</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> threadpool: <span class="type">ThreadPoolExecutor</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> availableCores =</span><br><span class="line">      <span class="keyword">if</span> (numUsableCores &gt; <span class="number">0</span>) numUsableCores <span class="keyword">else</span> <span class="type">Runtime</span>.getRuntime.availableProcessors()</span><br><span class="line">    <span class="keyword">val</span> numThreads = nettyEnv.conf.getInt(<span class="string">&quot;spark.rpc.netty.dispatcher.numThreads&quot;</span>,</span><br><span class="line">      math.max(<span class="number">2</span>, availableCores))</span><br><span class="line">    <span class="keyword">val</span> pool = <span class="type">ThreadUtils</span>.newDaemonFixedThreadPool(numThreads, <span class="string">&quot;dispatcher-event-loop&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">      pool.execute(<span class="keyword">new</span> <span class="type">MessageLoop</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    pool</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageLoop</span> <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> data = receivers.take()</span><br><span class="line">            <span class="keyword">if</span> (data == <span class="type">PoisonPill</span>) &#123;</span><br><span class="line">              <span class="comment">// Put PoisonPill back so that other MessageLoops can see it.</span></span><br><span class="line">              receivers.offer(<span class="type">PoisonPill</span>)</span><br><span class="line">              <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">              <span class="comment">// 开始处理消息</span></span><br><span class="line">            data.inbox.process(<span class="type">Dispatcher</span>.<span class="keyword">this</span>)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(e.getMessage, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit</span></span><br><span class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// Re-submit a MessageLoop so that Dispatcher will still work if</span></span><br><span class="line">            <span class="comment">// UncaughtExceptionHandler decides to not kill JVM.</span></span><br><span class="line">            threadpool.execute(<span class="keyword">new</span> <span class="type">MessageLoop</span>)</span><br><span class="line">          &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> t</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个基于Netty的消息系统</p>
<p>org.apache.spark.SparkEnv#createExecutorEnv:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createExecutorEnv</span></span>(</span><br><span class="line">      conf: <span class="type">SparkConf</span>,</span><br><span class="line">      executorId: <span class="type">String</span>,</span><br><span class="line">      hostname: <span class="type">String</span>,</span><br><span class="line">      numCores: <span class="type">Int</span>,</span><br><span class="line">      ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],</span><br><span class="line">      isLocal: <span class="type">Boolean</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line">          <span class="comment">// 初始化Executor的serializer,</span></span><br><span class="line">    <span class="comment">//      closureSerializer,</span></span><br><span class="line">    <span class="comment">//      serializerManager,</span></span><br><span class="line">    <span class="comment">//      mapOutputTracker,</span></span><br><span class="line">    <span class="comment">//      shuffleManager,</span></span><br><span class="line">    <span class="comment">//      broadcastManager,</span></span><br><span class="line">    <span class="comment">//      blockManager,</span></span><br><span class="line">    <span class="comment">//      securityManager,</span></span><br><span class="line">    <span class="comment">//      metricsSystem,</span></span><br><span class="line">    <span class="comment">//      memoryManager,</span></span><br><span class="line">    <span class="comment">//      outputCommitCoordinator组建，启动消息系统</span></span><br><span class="line">    <span class="keyword">val</span> env = create(</span><br><span class="line">      conf,</span><br><span class="line">      executorId,</span><br><span class="line">      hostname,</span><br><span class="line">      hostname,</span><br><span class="line">      <span class="type">None</span>,</span><br><span class="line">      isLocal,</span><br><span class="line">      numCores,</span><br><span class="line">      ioEncryptionKey</span><br><span class="line">    )</span><br><span class="line">    <span class="type">SparkEnv</span>.set(env)</span><br><span class="line">    env</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(</span><br><span class="line">      conf: <span class="type">SparkConf</span>,</span><br><span class="line">      executorId: <span class="type">String</span>,</span><br><span class="line">      bindAddress: <span class="type">String</span>,</span><br><span class="line">      advertiseAddress: <span class="type">String</span>,</span><br><span class="line">      port: <span class="type">Option</span>[<span class="type">Int</span>],</span><br><span class="line">      isLocal: <span class="type">Boolean</span>,</span><br><span class="line">      numUsableCores: <span class="type">Int</span>,</span><br><span class="line">      ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],</span><br><span class="line">      listenerBus: <span class="type">LiveListenerBus</span> = <span class="literal">null</span>,</span><br><span class="line">      mockOutputCommitCoordinator: <span class="type">Option</span>[<span class="type">OutputCommitCoordinator</span>] = <span class="type">None</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> isDriver = executorId == <span class="type">SparkContext</span>.<span class="type">DRIVER_IDENTIFIER</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Listener bus is only used on the driver</span></span><br><span class="line">    <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      assert(listenerBus != <span class="literal">null</span>, <span class="string">&quot;Attempted to create driver SparkEnv with null listener bus!&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> securityManager = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf, ioEncryptionKey)</span><br><span class="line">    <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      securityManager.initializeAuth()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ioEncryptionKey.foreach &#123; _ =&gt;</span><br><span class="line">      <span class="keyword">if</span> (!securityManager.isEncryptionEnabled()) &#123;</span><br><span class="line">        logWarning(<span class="string">&quot;I/O encryption enabled without RPC encryption: keys will be visible on the &quot;</span> +</span><br><span class="line">          <span class="string">&quot;wire.&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> systemName = <span class="keyword">if</span> (isDriver) driverSystemName <span class="keyword">else</span> executorSystemName</span><br><span class="line">    <span class="comment">// 创建一个基于Netty消息系统的分布式执行环境</span></span><br><span class="line">    <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, bindAddress, advertiseAddress, port.getOrElse(<span class="number">-1</span>), conf,</span><br><span class="line">      securityManager, numUsableCores, !isDriver)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> serializer = instantiateClassFromConf[<span class="type">Serializer</span>](</span><br><span class="line">      <span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.JavaSerializer&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">s&quot;Using serializer: <span class="subst">$&#123;serializer.getClass&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> serializerManager = <span class="keyword">new</span> <span class="type">SerializerManager</span>(serializer, conf, ioEncryptionKey)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> broadcastManager = <span class="keyword">new</span> <span class="type">BroadcastManager</span>(isDriver, conf, securityManager)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapOutputTracker = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">MapOutputTrackerMaster</span>(conf, broadcastManager, isLocal)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">MapOutputTrackerWorker</span>(conf)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Have to assign trackerEndpoint after initialization as MapOutputTrackerEndpoint</span></span><br><span class="line">    <span class="comment">// requires the MapOutputTracker itself</span></span><br><span class="line">    mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(<span class="type">MapOutputTracker</span>.<span class="type">ENDPOINT_NAME</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">MapOutputTrackerMasterEndpoint</span>(</span><br><span class="line">        rpcEnv, mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerMaster</span>], conf))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Let the user specify short names for shuffle managers</span></span><br><span class="line">    <span class="keyword">val</span> shortShuffleMgrNames = <span class="type">Map</span>(</span><br><span class="line">      <span class="string">&quot;sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName,</span><br><span class="line">      <span class="string">&quot;tungsten-sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName)</span><br><span class="line">    <span class="keyword">val</span> shuffleMgrName = conf.get(<span class="string">&quot;spark.shuffle.manager&quot;</span>, <span class="string">&quot;sort&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> shuffleMgrClass =</span><br><span class="line">      shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(<span class="type">Locale</span>.<span class="type">ROOT</span>), shuffleMgrName)</span><br><span class="line">    <span class="keyword">val</span> shuffleManager = instantiateClass[<span class="type">ShuffleManager</span>](shuffleMgrClass)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> useLegacyMemoryManager = conf.getBoolean(<span class="string">&quot;spark.memory.useLegacyMode&quot;</span>, <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> =</span><br><span class="line">      <span class="keyword">if</span> (useLegacyMemoryManager) &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">StaticMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> blockManagerPort = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      conf.get(<span class="type">DRIVER_BLOCK_MANAGER_PORT</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      conf.get(<span class="type">BLOCK_MANAGER_PORT</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> blockTransferService =</span><br><span class="line">      <span class="keyword">new</span> <span class="type">NettyBlockTransferService</span>(conf, securityManager, bindAddress, advertiseAddress,</span><br><span class="line">        blockManagerPort, numUsableCores)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(registerOrLookupEndpoint(</span><br><span class="line">      <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(rpcEnv, isLocal, conf, listenerBus)),</span><br><span class="line">      conf, isDriver)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line">    <span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(executorId, rpcEnv, blockManagerMaster,</span><br><span class="line">      serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,</span><br><span class="line">      blockTransferService, securityManager, numUsableCores)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> metricsSystem = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      <span class="comment">// Don&#x27;t start metrics system right now for Driver.</span></span><br><span class="line">      <span class="comment">// We need to wait for the task scheduler to give us an app ID.</span></span><br><span class="line">      <span class="comment">// Then we can start the metrics system.</span></span><br><span class="line">      <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">&quot;driver&quot;</span>, conf, securityManager)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We need to set the executor ID before the MetricsSystem is created because sources and</span></span><br><span class="line">      <span class="comment">// sinks specified in the metrics configuration file will want to incorporate this executor&#x27;s</span></span><br><span class="line">      <span class="comment">// ID into the metrics they report.</span></span><br><span class="line">      conf.set(<span class="string">&quot;spark.executor.id&quot;</span>, executorId)</span><br><span class="line">      <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">&quot;executor&quot;</span>, conf, securityManager)</span><br><span class="line">      ms.start()</span><br><span class="line">      ms</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> outputCommitCoordinator = mockOutputCommitCoordinator.getOrElse &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">OutputCommitCoordinator</span>(conf, isDriver)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> outputCommitCoordinatorRef = registerOrLookupEndpoint(<span class="string">&quot;OutputCommitCoordinator&quot;</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">OutputCommitCoordinatorEndpoint</span>(rpcEnv, outputCommitCoordinator))</span><br><span class="line">    outputCommitCoordinator.coordinatorRef = <span class="type">Some</span>(outputCommitCoordinatorRef)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> envInstance = <span class="keyword">new</span> <span class="type">SparkEnv</span>(</span><br><span class="line">      executorId,</span><br><span class="line">      rpcEnv,</span><br><span class="line">      serializer,</span><br><span class="line">      closureSerializer,</span><br><span class="line">      serializerManager,</span><br><span class="line">      mapOutputTracker,</span><br><span class="line">      shuffleManager,</span><br><span class="line">      broadcastManager,</span><br><span class="line">      blockManager,</span><br><span class="line">      securityManager,</span><br><span class="line">      metricsSystem,</span><br><span class="line">      memoryManager,</span><br><span class="line">      outputCommitCoordinator,</span><br><span class="line">      conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add a reference to tmp dir created by driver, we will delete this tmp dir when stop() is</span></span><br><span class="line">    <span class="comment">// called, and we only need to do it for driver. Because driver may run as a service, and if we</span></span><br><span class="line">    <span class="comment">// don&#x27;t delete this tmp dir when sc is stopped, then will create too many tmp dirs.</span></span><br><span class="line">    <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">      <span class="keyword">val</span> sparkFilesDir = <span class="type">Utils</span>.createTempDir(<span class="type">Utils</span>.getLocalDir(conf), <span class="string">&quot;userFiles&quot;</span>).getAbsolutePath</span><br><span class="line">      envInstance.driverTmpDir = <span class="type">Some</span>(sparkFilesDir)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    envInstance</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>RpcEnv初始化过程</p>
<ul>
<li><p>org.apache.spark.rpc.RpcEnv#create:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(</span><br><span class="line">    name: <span class="type">String</span>,</span><br><span class="line">    bindAddress: <span class="type">String</span>,</span><br><span class="line">    advertiseAddress: <span class="type">String</span>,</span><br><span class="line">    port: <span class="type">Int</span>,</span><br><span class="line">    conf: <span class="type">SparkConf</span>,</span><br><span class="line">    securityManager: <span class="type">SecurityManager</span>,</span><br><span class="line">    numUsableCores: <span class="type">Int</span>,</span><br><span class="line">    clientMode: <span class="type">Boolean</span>): <span class="type">RpcEnv</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> config = <span class="type">RpcEnvConfig</span>(conf, name, bindAddress, advertiseAddress, port, securityManager,</span><br><span class="line">    numUsableCores, clientMode)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">NettyRpcEnvFactory</span>().create(config)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>org.apache.spark.rpc.netty.NettyRpcEnvFactory#create</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(config: <span class="type">RpcEnvConfig</span>): <span class="type">RpcEnv</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = config.conf</span><br><span class="line">  <span class="comment">// Use JavaSerializerInstance in multiple threads is safe. However, if we plan to support</span></span><br><span class="line">  <span class="comment">// KryoSerializer in future, we have to use ThreadLocal to store SerializerInstance</span></span><br><span class="line">  <span class="keyword">val</span> javaSerializerInstance =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">JavaSerializer</span>(sparkConf).newInstance().asInstanceOf[<span class="type">JavaSerializerInstance</span>]</span><br><span class="line">     </span><br><span class="line">  <span class="comment">// 创建基于Netty框架的消息系统</span></span><br><span class="line">  <span class="keyword">val</span> nettyEnv =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">NettyRpcEnv</span>(sparkConf, javaSerializerInstance, config.advertiseAddress,</span><br><span class="line">      config.securityManager, config.numUsableCores)</span><br><span class="line">  <span class="keyword">if</span> (!config.clientMode) &#123;</span><br><span class="line">    <span class="keyword">val</span> startNettyRpcEnv: <span class="type">Int</span> =&gt; (<span class="type">NettyRpcEnv</span>, <span class="type">Int</span>) = &#123; actualPort =&gt;</span><br><span class="line">      <span class="comment">// 向外部系统注册RPC服务</span></span><br><span class="line">      nettyEnv.startServer(config.bindAddress, actualPort)</span><br><span class="line">      (nettyEnv, nettyEnv.address.port)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="type">Utils</span>.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        nettyEnv.shutdown()</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  nettyEnv</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>org.apache.spark.rpc.netty.Dispatcher#registerRpcEndpoint</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerRpcEndpoint</span></span>(name: <span class="type">String</span>, endpoint: <span class="type">RpcEndpoint</span>): <span class="type">NettyRpcEndpointRef</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> addr = <span class="type">RpcEndpointAddress</span>(nettyEnv.address, name)</span><br><span class="line">  <span class="keyword">val</span> endpointRef = <span class="keyword">new</span> <span class="type">NettyRpcEndpointRef</span>(nettyEnv.conf, addr, nettyEnv)</span><br><span class="line">  synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;RpcEnv has been stopped&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (endpoints.putIfAbsent(name, <span class="keyword">new</span> <span class="type">EndpointData</span>(name, endpoint, endpointRef)) != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;There is already an RpcEndpoint called <span class="subst">$name</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> data = endpoints.get(name)</span><br><span class="line">    endpointRefs.put(data.endpoint, data.ref)</span><br><span class="line">    receivers.offer(data)  <span class="comment">// for the OnStart message</span></span><br><span class="line">  &#125;</span><br><span class="line">  endpointRef</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>EndpointData初始化</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">EndpointData</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">      val name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      val endpoint: <span class="type">RpcEndpoint</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      val ref: <span class="type">NettyRpcEndpointRef</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> inbox = <span class="keyword">new</span> <span class="type">Inbox</span>(ref, endpoint)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[netty] <span class="class"><span class="keyword">class</span> <span class="title">Inbox</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    val endpointRef: <span class="type">NettyRpcEndpointRef</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val endpoint: <span class="type">RpcEndpoint</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Logging</span> &#123;</span><br><span class="line"></span><br><span class="line">  inbox =&gt;  <span class="comment">// Give this an alias so we can use it more clearly in closures.</span></span><br><span class="line"></span><br><span class="line">  <span class="meta">@GuardedBy</span>(<span class="string">&quot;this&quot;</span>)</span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">val</span> messages = <span class="keyword">new</span> java.util.<span class="type">LinkedList</span>[<span class="type">InboxMessage</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** True if the inbox (and its associated endpoint) is stopped. */</span></span><br><span class="line">  <span class="meta">@GuardedBy</span>(<span class="string">&quot;this&quot;</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> stopped = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Allow multiple threads to process messages at the same time. */</span></span><br><span class="line">  <span class="meta">@GuardedBy</span>(<span class="string">&quot;this&quot;</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> enableConcurrent = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** The number of threads processing messages for this inbox. */</span></span><br><span class="line">  <span class="meta">@GuardedBy</span>(<span class="string">&quot;this&quot;</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> numActiveThreads = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// OnStart should be the first message to process</span></span><br><span class="line">  inbox.synchronized &#123;</span><br><span class="line">    <span class="comment">// 向消息队列内添加OnStart事件</span></span><br><span class="line">    messages.add(<span class="type">OnStart</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>Executor消息系统接收到OnStart事件后向Driver注册自己的信息,</p>
<p>org.apache.spark.executor.CoarseGrainedExecutorBackend#onStart:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</span><br><span class="line">  logInfo(<span class="string">&quot;Connecting to driver: &quot;</span> + driverUrl)</span><br><span class="line">  rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">    <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">    driver = <span class="type">Some</span>(ref)</span><br><span class="line">    ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls))</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread).onComplete &#123;</span><br><span class="line">    <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Success</span>(msg) =&gt;</span><br><span class="line">      <span class="comment">// Always receive `true`. Just ignore it</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">s&quot;Cannot register with driver: <span class="subst">$driverUrl</span>&quot;</span>, e, notifyDriver = <span class="literal">false</span>)</span><br><span class="line">  &#125;(<span class="type">ThreadUtils</span>.sameThread)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Driver接收到RegisterExecutor事件的处理，org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#receiveAndReply：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">RegisterExecutor</span>(executorId, executorRef, hostname, cores, logUrls) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (executorDataMap.contains(executorId)) &#123;</span><br><span class="line">          executorRef.send(<span class="type">RegisterExecutorFailed</span>(<span class="string">&quot;Duplicate executor ID: &quot;</span> + executorId))</span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (scheduler.nodeBlacklist.contains(hostname)) &#123;</span><br><span class="line">          <span class="comment">// If the cluster manager gives us an executor on a blacklisted node (because it</span></span><br><span class="line">          <span class="comment">// already started allocating those resources before we informed it of our blacklist,</span></span><br><span class="line">          <span class="comment">// or if it ignored our blacklist), then we reject that executor immediately.</span></span><br><span class="line">          logInfo(<span class="string">s&quot;Rejecting <span class="subst">$executorId</span> as it has been blacklisted.&quot;</span>)</span><br><span class="line">          executorRef.send(<span class="type">RegisterExecutorFailed</span>(<span class="string">s&quot;Executor is blacklisted: <span class="subst">$executorId</span>&quot;</span>))</span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// If the executor&#x27;s rpc env is not listening for incoming connections, `hostPort`</span></span><br><span class="line">          <span class="comment">// will be null, and the client connection should be used to contact the executor.</span></span><br><span class="line">          <span class="keyword">val</span> executorAddress = <span class="keyword">if</span> (executorRef.address != <span class="literal">null</span>) &#123;</span><br><span class="line">              executorRef.address</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              context.senderAddress</span><br><span class="line">            &#125;</span><br><span class="line">          logInfo(<span class="string">s&quot;Registered executor <span class="subst">$executorRef</span> (<span class="subst">$executorAddress</span>) with ID <span class="subst">$executorId</span>&quot;</span>)</span><br><span class="line">          addressToExecutorId(executorAddress) = executorId</span><br><span class="line">          totalCoreCount.addAndGet(cores)</span><br><span class="line">          totalRegisteredExecutors.addAndGet(<span class="number">1</span>)</span><br><span class="line">          <span class="keyword">val</span> data = <span class="keyword">new</span> <span class="type">ExecutorData</span>(executorRef, executorAddress, hostname,</span><br><span class="line">            cores, cores, logUrls)</span><br><span class="line">          <span class="comment">// This must be synchronized because variables mutated</span></span><br><span class="line">          <span class="comment">// in this block are read when requesting executors</span></span><br><span class="line">          <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">            executorDataMap.put(executorId, data)</span><br><span class="line">            <span class="keyword">if</span> (currentExecutorIdCounter &lt; executorId.toInt) &#123;</span><br><span class="line">              currentExecutorIdCounter = executorId.toInt</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (numPendingExecutors &gt; <span class="number">0</span>) &#123;</span><br><span class="line">              numPendingExecutors -= <span class="number">1</span></span><br><span class="line">              logDebug(<span class="string">s&quot;Decremented number of pending executors (<span class="subst">$numPendingExecutors</span> left)&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">            <span class="comment">// 向Executor发送RegisteredExecutor事件</span></span><br><span class="line">          executorRef.send(<span class="type">RegisteredExecutor</span>)</span><br><span class="line">          <span class="comment">// Note: some tests expect the reply to come after we put the executor in the map</span></span><br><span class="line">          context.reply(<span class="literal">true</span>)</span><br><span class="line">          listenerBus.post(</span><br><span class="line">            <span class="type">SparkListenerExecutorAdded</span>(<span class="type">System</span>.currentTimeMillis(), executorId, data))</span><br><span class="line">          makeOffers()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>Executor接收到Driver回复的RegisteredExecutor消息后的处理：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisteredExecutor</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;Successfully registered with driver&quot;</span>)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">//向Driver注册成功后,创建Executor</span></span><br><span class="line">        executor = <span class="keyword">new</span> <span class="type">Executor</span>(executorId, hostname, env, userClassPath, isLocal = <span class="literal">false</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">          exitExecutor(<span class="number">1</span>, <span class="string">&quot;Unable to create executor due to &quot;</span> + e.getMessage, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterExecutorFailed</span>(message) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">&quot;Slave registration failed: &quot;</span> + message)</span><br><span class="line">    <span class="comment">// 接收LaunchTask中的TaskSet,进行处理</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> taskDesc = <span class="type">TaskDescription</span>.decode(data.value)</span><br><span class="line">        logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">        executor.launchTask(<span class="keyword">this</span>, taskDesc)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<p>至此，spark的AM、Executor都已经成功启动，等待用户程序被解析成DAG后，生成TaskSet交由Executor开始执行。</p>
</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
              <a href="/tags/yarn/" rel="tag"># yarn</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/17/Spark-collect%E6%96%B9%E6%B3%95%E5%88%86%E6%9E%90-Executor%E7%AB%AF/" rel="prev" title="Spark collect方法分析-Executor端">
      <i class="fa fa-chevron-left"></i> Spark collect方法分析-Executor端
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/05/18/PVE%E4%B8%8B%E4%B8%BALCX%E6%B7%BB%E5%8A%A0TUN%E6%8E%A5%E5%8F%A3/" rel="next" title="PVE8.0下为LCX添加TUN接口">
      PVE8.0下为LCX添加TUN接口 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">过程分析</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Wang Fanming</p>
  <div class="site-description" itemprop="description">吾生也有涯，而知也无涯，以有涯随无涯，殆己！已而为知者，殆而已矣。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wangfanming" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wangfanming" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/wangfanming1204@gmail.com" title="E-Mail → wangfanming1204@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Fanming</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">151k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:18</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
