<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>keepalived入门</title>
    <url>/2019/03/26/Keepalived%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h1 id="keepalived入门"><a href="#keepalived入门" class="headerlink" title="keepalived入门"></a>keepalived入门</h1><h2 id="keepalived简介"><a href="#keepalived简介" class="headerlink" title="keepalived简介"></a>keepalived简介</h2><blockquote>
<p>&emsp;&emsp;keepalived是集群管理中保证集群高可用的一个服务软件，用来防止单点故障。</p>
</blockquote>
<blockquote>
<p>&emsp;&emsp;keepalived的作用是检测web服务器的状态，如果有一台web服务器死机，<br>或工作出现故障，keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器<br>工作正常后，keepalived自动将web服务器加入到服务器集群中，这些工作全部自动完成，不需要<br>人工干涉，需要人工做的只是修复故障的web服务器。</p>
</blockquote>
<blockquote>
<p>&emsp;&emsp;keepalived是以VRRP协议位实现基础的，VRRP全称 Virtual Router<br>Redundancy Protocol，即虚拟路由冗余协议。</p>
</blockquote>
<blockquote>
<p>&emsp;&emsp;虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同<br>功能的路由器组成的一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务<br>的VIP（Virtual IPAddress,虚拟IP地址，该路由器所在局域网内其他机器的默认路由为该VIP），<br>master会发组播，当backup收不到VRRP包时，就认为master宕掉了，这时就需要根据VRRP的优先级来<br>就可以保证路由器的高可用了。</p>
</blockquote>
<span id="more"></span>

<h3 id="keepalived核心内容"><a href="#keepalived核心内容" class="headerlink" title="keepalived核心内容"></a>keepalived核心内容</h3><blockquote>
<p>&emsp;&emsp;keepalived主要有三个模块，分别是core、check和VRRP。core模块为<br>keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查<br>，包括常见的各种检查。VRRP模块是来实现VRRP协议的。</p>
</blockquote>
<p>初始状态：</p>
<blockquote>
</blockquote>
<p><img src="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/image/keep1.bmp"></p>
<p>主机状态：</p>
<blockquote>
</blockquote>
<p><img src="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/image/keep2.bmp"></p>
<p>主机恢复：</p>
<blockquote>
</blockquote>
<p><img src="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/image/keep3.bmp"></p>
]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark collect方法调用分析</title>
    <url>/2023/12/12/Spark%20collect%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>通过对调用过程的分析，深入理解整个框架的计算过程，从根本上理解程序的执行逻辑。</p>
<span id="more"></span>


<ul>
<li><p>方法调用链分析：</p>
<ol>
<li>进入org.apache.spark.rdd.RDD#collect()</li>
</ol>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def <span class="title function_">collect</span><span class="params">()</span>: Array[T] = withScope &#123;</span><br><span class="line">  <span class="type">val</span> <span class="variable">results</span> <span class="operator">=</span> sc.runJob(<span class="built_in">this</span>, (iter: Iterator[T]) =&gt; iter.toArray)</span><br><span class="line">  Array.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>上述withScope是一个柯里化操作，方法体内为一个匿名函数，这样做是为了在任务提交前对SparkContext做一些设置。</li>
</ul>
<ol start="2">
<li>org.apache.spark.SparkContext#runJob方法：</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="comment">// 默认的分区数等于RDD的分区数。</span></span><br><span class="line">  runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>org.apache.spark.SparkContext#runJob方法经过多次重载调用，</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</span><br><span class="line">  runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</span><br><span class="line">  results</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>(index, res) &#x3D;&gt; results(index) &#x3D; res 该函数表示将每个分区的计算结果存放在数组results相应的索引上。</li>
</ul>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  ...</span><br><span class="line">    <span class="comment">//对func提前预设闭包清除</span></span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line"> ...</span><br><span class="line">    <span class="comment">//调用DAGScheduler进行DAG构建</span></span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br><span class="line">  <span class="comment">// 对需要进行保存的RDD进行保存</span></span><br><span class="line">  rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>org.apache.spark.scheduler.DAGScheduler#runJob</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">  <span class="comment">//提交至消息总线，由DAGScheduler.doOnReceive处理</span></span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line">  <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">  <span class="comment">// 方法阻塞等待任务执行结束</span></span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>org.apache.spark.scheduler.DAGScheduler#submitJob</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">...</span><br><span class="line">  <span class="comment">// 生成全局的JobId</span></span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">  <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// Return immediately if the job is running 0 tasks</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">  <span class="comment">// 分区数等于task数</span></span><br><span class="line">  <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">  <span class="comment">// 向总线发出JobSubmitted事件，通过监听器模式，通知给所有监听器</span></span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">  waiter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">      finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      listener: <span class="type">JobListener</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">      <span class="comment">// 从最后一个Stage开始向前创建DAG</span></span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">BarrierJobSlotsNumberCheckFailed</span> =&gt;</span><br><span class="line">  ...</span><br><span class="line">        <span class="keyword">val</span> numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">          <span class="keyword">new</span> <span class="type">BiFunction</span>[<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>] &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(key: <span class="type">Int</span>, value: <span class="type">Int</span>): <span class="type">Int</span> = value + <span class="number">1</span></span><br><span class="line">          &#125;)</span><br><span class="line">        <span class="keyword">if</span> (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">          messageScheduler.schedule(</span><br><span class="line">            <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">              <span class="comment">// job提交失败，在最大允许次数内进行重新提交</span></span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, finalRDD, func,</span><br><span class="line">                partitions, callSite, listener, properties))</span><br><span class="line">...</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Job submitted, clear internal data.</span></span><br><span class="line">    barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line">	...</span><br><span class="line">    <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    <span class="comment">// 任务提交流程 - 根据jobId获取其对应的所有Stages,通知给消息总线，</span></span><br><span class="line">    <span class="comment">// 在org.apache.spark.status.AppStatusListener.onJobStart方法中处理</span></span><br><span class="line">    <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    <span class="comment">// 根据SparkListenerJobStart事件，构建stage的DAG执行图,并刷新至UI</span></span><br><span class="line">    listenerBus.post(</span><br><span class="line">      <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">    <span class="comment">// 提交Stage开始执行</span></span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>SparkListenerJobStart事件通过消息总线会通知到org.apache.spark.status.AppStatusListener#onJobStart，进行UI界面展示。</li>
</ul>
<ol start="7">
<li>org.apache.spark.scheduler.DAGScheduler#submitStage</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">      <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="comment">// 通过查找missingStage构建出了所有的Stage,由于Stage从后向前构建的特性，</span></span><br><span class="line">      <span class="comment">// flinaStage的missingStage就是所有的直接依赖的Stage</span></span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="comment">// 只有最后一个Stage,即最贴近数据源的Stage才满足missing.isEmpty</span></span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        <span class="comment">// 开始从最后一个Stage提交Task</span></span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>这是一个尾递归方法，由第一个Stage触发上游依赖的Stage的构建，从而完成DAG的构建。</li>
</ul>
<ol start="8">
<li>org.apache.spark.scheduler.DAGScheduler#getMissingParentStages</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">  <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="comment">// We are manually maintaining a stack here to prevent StackOverflowError</span></span><br><span class="line">  <span class="comment">// caused by recursively visiting</span></span><br><span class="line">  <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ArrayStack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">      visited += rdd</span><br><span class="line">      <span class="comment">// 查找缓存中的RDD的分区的TaskLocation,没有就通过BlockManagerMaster进行获取并存储</span></span><br><span class="line">      <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">      <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">        <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">          dep <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">              <span class="comment">// 构建ShuffleMapStage，优先使用已经创建过的，复用已经生成的ShuffleMapStage的输出</span></span><br><span class="line">              <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">              <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</span><br><span class="line">                missing += mapStage</span><br><span class="line">              &#125;</span><br><span class="line">            <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</span><br><span class="line">              waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  waitingForVisit.push(stage.rdd)</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    visit(waitingForVisit.pop())</span><br><span class="line">  &#125;</span><br><span class="line">  missing.toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="9">
<li>org.apache.spark.scheduler.DAGScheduler#submitMissingTasks</li>
</ol>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Called when stage&#x27;s parents are available and we can now do its task. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">  logDebug(<span class="string">&quot;submitMissingTasks(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">  <span class="comment">// with this Stage</span></span><br><span class="line">  <span class="keyword">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class="line"></span><br><span class="line">  runningStages += stage</span><br><span class="line">  <span class="comment">// SparkListenerStageSubmitted should be posted before testing whether tasks are</span></span><br><span class="line">  <span class="comment">// serializable. If tasks are not serializable, a SparkListenerStageCompleted event</span></span><br><span class="line">  <span class="comment">// will be posted, which should always come after a corresponding SparkListenerStageSubmitted</span></span><br><span class="line">  <span class="comment">// event.</span></span><br><span class="line">  stage <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(</span><br><span class="line">        stage = s.id, maxPartitionId = s.rdd.partitions.length - <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">          (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">        &#125;.toMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">      listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If there are tasks to execute, record the submission time of the stage. Otherwise,</span></span><br><span class="line">  <span class="comment">// post the even without the submission time, which indicates that this stage was</span></span><br><span class="line">  <span class="comment">// skipped.</span></span><br><span class="line">  <span class="keyword">if</span> (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">    stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">  &#125;</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.</span></span><br><span class="line">  <span class="comment">// Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast</span></span><br><span class="line">  <span class="comment">// the serialized copy of the RDD and for each task we will deserialize it, which means each</span></span><br><span class="line">  <span class="comment">// task gets a different copy of the RDD. This provides stronger isolation between tasks that</span></span><br><span class="line">  <span class="comment">// might modify state of objects referenced in their closures. This is necessary in Hadoop</span></span><br><span class="line">  <span class="comment">// where the JobConf/Configuration object is not thread-safe.</span></span><br><span class="line">  <span class="keyword">var</span> taskBinary: <span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Byte</span>]] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> partitions: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).</span></span><br><span class="line">    <span class="comment">// For ResultTask, serialize and broadcast (rdd, func).</span></span><br><span class="line">    <span class="keyword">var</span> taskBinaryBytes: <span class="type">Array</span>[<span class="type">Byte</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// taskBinaryBytes and partitions are both effected by the checkpoint status. We need</span></span><br><span class="line">    <span class="comment">// this synchronization in case another concurrent job is checkpointing this RDD, so we get a</span></span><br><span class="line">    <span class="comment">// consistent view of both variables.</span></span><br><span class="line">    <span class="type">RDDCheckpointData</span>.synchronized &#123;</span><br><span class="line">      taskBinaryBytes = stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(</span><br><span class="line">            closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class="type">AnyRef</span>))</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class="type">AnyRef</span>))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      partitions = stage.rdd.partitions</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 通过广播将闭包序列化后的task信息广播出去</span></span><br><span class="line">    taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="comment">// In the case of a failure during serialization, abort the stage.</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">&quot;Task not serializable: &quot;</span> + e.toString, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Abort execution</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task serialization failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Abort execution</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 拆解Stage，提取各分区最优计算位置，封装成task集合</span></span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">          stage.pendingPartitions += id</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">            stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    logInfo(<span class="string">s&quot;Submitting <span class="subst">$&#123;tasks.size&#125;</span> missing tasks from <span class="subst">$stage</span> (<span class="subst">$&#123;stage.rdd&#125;</span>) (first 15 &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;tasks are for partitions <span class="subst">$&#123;tasks.take(15).map(_.partitionId)&#125;</span>)&quot;</span>)</span><br><span class="line">    <span class="comment">// 将task集合构建为TaskSet提交给taskScheduler进行计算</span></span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">      tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Because we posted SparkListenerStageSubmitted earlier, we should mark</span></span><br><span class="line">    <span class="comment">// the stage as completed here in case there are no tasks to run</span></span><br><span class="line">    markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,&quot;</span> +</span><br><span class="line">            <span class="string">s&quot;available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,&quot;</span> +</span><br><span class="line">            <span class="string">s&quot;partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span>)</span><br><span class="line">        markMapStageJobsAsFinished(stage)</span><br><span class="line">      <span class="keyword">case</span> stage : <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 当前Stage计算完成后，开始提交其子Stage进行计算</span></span><br><span class="line">    submitWaitingChildStages(stage)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Zookeeper相关知识点</title>
    <url>/2023/12/12/Zookeeper%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Zookeeper相关知识点"><a href="#Zookeeper相关知识点" class="headerlink" title="Zookeeper相关知识点"></a>Zookeeper相关知识点</h1><h4 id="Zookeeper概述"><a href="#Zookeeper概述" class="headerlink" title="Zookeeper概述"></a>Zookeeper概述</h4><p>&emsp;&emsp;Zookeeper是一个<strong>分布式协调服务</strong>的开源框架。主要用来解决分布式集群中应用系统<br>的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。</p>
<p>&emsp;&emsp;Zookeeper本质上是一个分布式的小文件系统。提供基于类似文件系统的<strong>目录树</strong>方式的数据<br>存储，并且可以对树中的节点进行有效管理。从而用来维护和监控<strong>储存的数据的状态的变化</strong>。通过监控<br>这些数据状态的变化，从而可以达到基于数据的集群管理。提供诸如：统一命名服务、分布式配置管理、分布式消息队列、<br>分布式锁、分布式协调等功能。</p>
<span id="more"></span>

<p><strong>Zookeeper集群角色</strong></p>
<ul>
<li><p>Leader ： Zookeeper集群工作的核心，是集群的事务(写操作)请求的唯一调度和处理者，保证集群事务处理的顺序性。<br>  同时也是集群内部各个服务的调度者。</p>
</li>
<li><p>Follower：处理客户端的<strong>非事务</strong>请求，转发事务请求给Leader，参与集群Leader选举投票。</p>
</li>
<li><p>Observer：充当观察者角色，观察Zookeeper集群的最新状态变化并将这些状态同步过来，其对于非事务请求可以进行独立<br>处理，对于事务请求，则会转发给Leader，不会参与任何形式的投票，通常用于在不影响集群事务处理能力的前提下<br>提升集群的非事务处理能力。</p>
</li>
</ul>
<p><strong>Zookeeper特性</strong></p>
<ul>
<li>全局一致性：每个server保存一份相同的数据副本，client无论连接到哪个server，展示的数据都是一致的。</li>
<li>可靠性：如果消息被其中一台服务器接收，那么将被所有的服务器接收。</li>
<li>顺序性：包括全局有序和偏序两种。</li>
<li>数据更新原子性：一次数据更新要么成功，要么失败，不存在中间状态。</li>
<li>实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效信息。</li>
</ul>
<blockquote>
<p><strong>全局有序</strong>是指如果在一台服务器上消息 a 在消息 b 前发布，则在所有 Server 上消息 a<br>都将在消息 b 前被发布。</p>
</blockquote>
<blockquote>
<p><strong>偏序</strong>是指如果一个消息 b 在消息 a 后被同一个发送者发布， a 必<br> 将排在 b 前面。</p>
</blockquote>
<hr>
<h4 id="Zookeeper-shell"><a href="#Zookeeper-shell" class="headerlink" title="Zookeeper shell"></a>Zookeeper shell</h4><p>&emsp;&emsp;运行<code>zkCli.sh -server ip</code> 进入命令行工具。</p>
<p>&emsp;&emsp;输入<code>help</code>,输出如下提示：</p>
<p><img src="https://github.com/wangfanming/blog/blob/467eedb713de54c813589293a487504a59304a36/image/zookeeper_shell_help.bmp"></p>
<p>带有<code>[watch]</code>的表示<br>1、shell基本操作</p>
<p><strong>创建节点</strong>：</p>
<p><code>create [-s] [-e] path data ac</code></p>
<p>其中，<code>-s</code>或<code>-e</code>分别指定节点特性，顺序或临时节点，若不指定，则表示创建为持久节点，<code>acl</code>用来进行权限控制。</p>
<p><strong>读取节点</strong>：</p>
<p><code>ls path [watch]</code></p>
<p><code>get path [watch]</code></p>
<p><code>ls2 path [watch]</code></p>
<p>&emsp;&emsp;与读相关的命令有<code>ls</code>命令和<code>get</code>命令，<code>ls</code>命令用来列出Zookeeper指定节点下的所有子节点，只能查看<br>指定节点下的<strong>第一级</strong>的所有子节点；<code>get</code>命令可以获取Zookeeper指定节点的数据内容和属性信息。</p>
<p><strong>更新节点</strong></p>
<p><code>set path [watch]</code></p>
<p><code>data</code>就是要更新的内容，version表示数据版本。</p>
<p><strong>删除节点</strong></p>
<p><code>delete path [watch]</code></p>
<p>若删除的节点存在子节点，那么无法删除节点，必须先删除子节点，再删除父节点。<br>也可以使用<code>rmr path</code>,递归强制删除。</p>
<p><strong>对节点添加限制(弱限制)</strong></p>
<p><code>setquota -|-b val path</code> 对节点增加限制。</p>
<ul>
<li><code>n</code>:表示子节点的最大个数。</li>
<li><code>b</code>:表示数据值的最大长度。</li>
<li><code>val</code>:子节点最大格式或数据值的最大长度。</li>
<li><code>path</code>:节点路径。</li>
</ul>
<p><strong>其他命令</strong></p>
<p><code>history</code>:列出命令历史<br><code>redo</code>:改命令可以重新执行指定命令Bain好的历史命令，通常结合<code>history</code>一起使用。</p>
<hr>
<h4 id="Zookeeper数据模型"><a href="#Zookeeper数据模型" class="headerlink" title="Zookeeper数据模型"></a>Zookeeper数据模型</h4><p>&emsp;&emsp;Zookeeper采用<strong>树形层次结构</strong>，每个节点被称为<code>Znode</code>。</p>
<p>1、<code>Znode</code>节点的特性：</p>
<ul>
<li>Znode兼具文件和目录两种特点。既像文件一样维护着数据、元信息、ACL(访问权限列表)、时间戳等<br>  数据结构，又像目录一样可以作为路径标识的一部分，并可以具有子<code>Znode</code>。</li>
<li>Znode具有原子性操作。</li>
<li>Znode存储数据大小有限制。通常以<code>KB</code>为单位。</li>
<li>Znode通过路径引用。<strong>路径必须是绝对的</strong>。</li>
</ul>
<p>每个Znode节点都是由3部分组成：</p>
<ul>
<li><code>stat</code>:此为状态信息，描述Znode的版本、权限等信息。</li>
<li><code>data</code>:与该Znode关联的数据。</li>
<li><code>children</code>:该Znode下的子节点。</li>
</ul>
<p>2、节点类型</p>
<p>&emsp;&emsp;Znode有两种，分别是<strong>临时节点</strong>和<strong>永久节点</strong>。节点类型在创建时即被确定，且不能改变。</p>
<p><strong>临时节点</strong>：该节点的生命周期依赖于创建它们的会话。会话一旦结束，临时节点就会被自动删除。临时节点不允许有子节点。<br><strong>永久节点</strong>：该节点的生命周期不依赖会话，并且只有在客户端显式删除时，它们才会被删除。</p>
<p>Znode还有一个序列化的特性，如果创建的时候指定的话，该Znode的名字后面会自动追加一个不断增加的序列号。<br>用来记录每个子节点创建的先后顺序。格式：<code>%10d</code>(10位数字，不够用0补充)</p>
<p>这样就存在了四种类型的<code>Znode</code>节点：</p>
<ul>
<li><code>PERSISTENT</code>:永久节点</li>
<li><code>EPHEMERAL</code>:临时节点</li>
<li><code>PERSISTENT_SEQUENTIAL</code>:永久、序列化节点</li>
<li><code>EPHEMERAL_SEQUENTIAL</code>:临时、序列化节点</li>
</ul>
<p>3、节点属性</p>
<p>&emsp;&emsp;每个Znode都包含了一系列的属性。</p>
<ul>
<li><code>dataVersion</code>:数据版本号。每次对数据的<code>set</code>操作都会使其自加1，可以有效避免数据更新时的先后顺序问题。</li>
<li><code>cversion</code>:子节点的版本号。当<code>Znode</code>的子节点有变化时，其值都会加1。</li>
<li><code>aclVersion</code>:ACL的版本号。</li>
<li><code>cZid</code>:创建Znode的事务的ID。</li>
<li><code>mZid</code>:Znode被修改的事务ID。</li>
<li><code>ctime</code>:节点创建时的时间戳。</li>
<li><code>mtime</code>:节点最新一次更新发生的时间。</li>
<li><code>ephemeralOwner</code>:如果该节点为临时节点，其值表示与该节点绑定的session id，如果不是则为0。</li>
</ul>
<p>在client和server通信之前，首先需要建立连接，该连接称为session。连接建立后如果发生超时、授权失败，或显式的关闭连接，<br>连接便处于CLOSED状态，此时session结束。</p>
<p>4、Zookeeper的Watch机制</p>
<p>&emsp;&emsp;Watch说的是Zookeeper的监听机制。<strong>一个Watch事件是一个一次性的触发器</strong>，当被设置了Watch<br>的数据发生了改变的时候，则服务器将这一改变打送给设置了Watch的客户端，以便通知它们。</p>
<p>&emsp;&emsp;改变有很多种方式，如：节点创建、节点删除、节点改变、子节点改变等。</p>
<p><strong>Watch机制的特点</strong></p>
<ul>
<li>一次性触发。</li>
<li>Watcher event异步发送。即watcher的通知事件从server到client是异步的。</li>
<li>数据监视。<code>getdata()</code>、<code>exists()</code>设置了数据监视。<code>getchildren()</code>设置了子节点监视。</li>
<li>注册watcher。<code>getData()</code>,<code>exists()</code>,<code>getchildren()</code></li>
<li>触发watcher。<code>create()</code>,<code>delete()</code>,<code>setData()</code></li>
</ul>
<p>5、Watch的事件类型</p>
<p>Zookeeper的watch实际上要处理两类事件。</p>
<ul>
<li>连接状态事件(type&#x3D;None,path&#x3D;null)</li>
</ul>
<p><code>None</code>:在客户端与Zookeeper集群中的服务器断开连接的时候，客户端会收到这个事件。</p>
<ul>
<li>节点事件</li>
</ul>
<p><code>NodeCreated</code>：Znode创建事件</p>
<p><code>NodeDeleted</code>：Znode删除事件</p>
<p><code>NodeDataChanged</code>：Znode数据内容更新事件。其实本质上该事件只关注dataVersion版本号，但是只要<br>调用了更新接口，<code>dataVersion</code>就会有变更。</p>
<p><code>NodeChildrenChanged</code>：Znode子节点改变事件，只关注<strong>子节点的个数</strong>变更，子节点内容变更是不会通知的。</p>
<hr>
<h4 id="Zookeeper客户端API"><a href="#Zookeeper客户端API" class="headerlink" title="Zookeeper客户端API"></a>Zookeeper客户端API</h4><p>&emsp;&emsp;<code>org.apache.zookeeper.Zookeeper</code> 是客户端入口主类，负责建立与 server<br>的会话， 它供以下几类主要方法：</p>
<ul>
<li><code>create</code>:在本地目录树中创建一个节点。</li>
<li><code>delete</code>:删除一个节点。</li>
<li><code>exists</code>:测试本地是否存在目标节点。</li>
<li><code>get/set data</code>:从目标节点上读取&#x2F;写 数据。</li>
<li><code>getChildren</code>:检索一个子节点上的列表。</li>
</ul>
<p>基本使用与模板代码：<br>1、引入pom依赖：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;3.4.9&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>2、模板代码：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception &#123;</span><br><span class="line">// 初始化 ZooKeeper 实例(zk 地址、会话超时时间，与系统默认一致、 watcher)</span><br><span class="line">ZooKeeper zk = new ZooKeeper(&quot;node-21:2181,node-22:2181&quot;, 30000, new Watcher() &#123;</span><br><span class="line">@Override</span><br><span class="line">public void process(WatchedEvent event) &#123;</span><br><span class="line">System.out.println(&quot;已经触发了&quot; + event.getType() + &quot;事件！ &quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line">// 创建一个目录节点</span><br><span class="line">zk.create(&quot;/testRootPath&quot;, &quot;testRootData&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">CreateMode.PERSISTENT);</span><br><span class="line">// 创建一个子目录节点</span><br><span class="line">zk.create(&quot;/testRootPath/testChildPathOne&quot;, &quot;testChildDataOne&quot;.getBytes(),</span><br><span class="line">Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);</span><br><span class="line">System.out.println(new String(zk.getData(&quot;/testRootPath&quot;,false,null)));</span><br><span class="line">// 取出子目录节点列表</span><br><span class="line">System.out.println(zk.getChildren(&quot;/testRootPath&quot;,true));</span><br><span class="line">// 修改子目录节点数据</span><br><span class="line">zk.setData(&quot;/testRootPath/testChildPathOne&quot;,&quot;modifyChildDataOne&quot;.getBytes(),-1);</span><br><span class="line">System.out.println(&quot;目录节点状态： [&quot;+zk.exists(&quot;/testRootPath&quot;,true)+&quot;]&quot;);</span><br><span class="line">// 创建另外一个子目录节点</span><br><span class="line">zk.create(&quot;/testRootPath/testChildPathTwo&quot;, &quot;testChildDataTwo&quot;.getBytes(),</span><br><span class="line">Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT);</span><br><span class="line">System.out.println(new String(zk.getData(&quot;/testRootPath/testChildPathTwo&quot;,true,null)));</span><br><span class="line">// 删除子目录节点</span><br><span class="line">zk.delete(&quot;/testRootPath/testChildPathTwo&quot;,-1);</span><br><span class="line">zk.delete(&quot;/testRootPath/testChildPathOne&quot;,-1);</span><br><span class="line">// 删除父目录节点</span><br><span class="line">zk.delete(&quot;/testRootPath&quot;,-1);</span><br><span class="line">zk.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Zookeeper典型应用"><a href="#Zookeeper典型应用" class="headerlink" title="Zookeeper典型应用"></a>Zookeeper典型应用</h4><p>1、数据发布与订阅</p>
<blockquote>
<p>&emsp;&emsp;发布与订阅模型，即所谓的配置中心。发布者将数据发布到ZK节点上，供订阅者动态获取数据，<br>    实现配置信息的集中式管理和动态更新。</p>
</blockquote>
<p>   &emsp;&emsp;<strong>适合数据量很小的场景</strong>，这样数据更新可能会比较快。</p>
<p>2、命名服务</p>
<blockquote>
<p>&emsp;&emsp;在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，<br>提供者等信息。被命名的实体通常可以是集群中的机器，提供服务的地址，远程对象等等，这些都可以通称为名字(Name)。<br>通过ZK提供的创建节点的API，能够很容易地创建一个全局唯一的path，这个path就可以作为一个名称。</p>
</blockquote>
<p>阿里巴巴集团开源的分布式服务框架 Dubbo 中使用 ZooKeeper 来作为其命<br>名服务，维护全局的服务地址列表。</p>
<p>3、分布式锁</p>
<blockquote>
<p>&emsp;&emsp;分布式锁，这个主要得益于Zookeeper保证了数据的强一致性。锁服务可以分为两类，一个是保持独占，另一个是控制时序。</p>
</blockquote>
<ul>
<li><p>保持独占：就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是一个<code>Znode</code>看作是一把锁，通过<code>create znode</code>的方式实现。</p>
</li>
<li><p>控制时序：就是所有试图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序。做法和上面基本类似，只是这里 &#x2F;distribute_lock 已经<br>预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL 来指定）。Zk 的父节点（&#x2F;distribute_lock）<br>维持一份 sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>Java WEB项目中中文乱码</title>
    <url>/2017/05/12/%E5%85%B3%E4%BA%8EJavaWEB%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="Java-WEB项目中中文乱码的解决方法"><a href="#Java-WEB项目中中文乱码的解决方法" class="headerlink" title="Java WEB项目中中文乱码的解决方法"></a>Java WEB项目中中文乱码的解决方法</h1><hr>
<h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>如下表单：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;form action=&quot;$&#123;pageContext.request.contextPath&#125;/UserServlet&quot; method=&quot;post&quot;&gt;</span><br><span class="line">    姓名：&lt;input type=&quot;text&quot; name=&quot;name&quot;&gt;</span><br><span class="line">    &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;</span><br><span class="line">&lt;/form&gt;    </span><br></pre></td></tr></table></figure>
<p>在这个表单中如果‘姓名’为中文，提交至web后端进行获取，如：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">name = request.getParameter(&quot;name&quot;);</span><br></pre></td></tr></table></figure>
<p>此时获取到的name的值一定是乱码，此时就要进行中文乱码处理了。</p>
<h2 id="处理方案可根据请求的类型分为两类"><a href="#处理方案可根据请求的类型分为两类" class="headerlink" title="处理方案可根据请求的类型分为两类:"></a>处理方案可根据请求的类型分为两类:</h2><p><strong>POST请求</strong>：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">request.setCharacterEncoding(&quot;UTF-8&quot;);//在接收请求参数之前，将request的缓冲区字符集设置为UTF-8</span><br><span class="line">request.getParameter(&quot;name&quot;);//此时再获取表单中的中文数据就不会乱码了</span><br></pre></td></tr></table></figure>
<p><strong>GET请求</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">String param = reques.getParameter(&quot;name&quot;);//首先获取表单数据</span><br><span class="line">String name = new String(param.getBytes(&quot;ISO-8859-1&quot;),&quot;UTF-8&quot;);//将获取到的表单数据以ISO-8859-1字符集转成字节数组，再按照UTF-8字符集编码成新的字符串，即可解决中文乱码</span><br></pre></td></tr></table></figure>

<p><strong>重点：</strong><br>    表单数据之所以乱码，是因为字符集不匹配，导致的，在Post请求时，我们在获取请求参数之前，将request缓冲区的字符集设置成UTF-8，即可在表单数据发送过来时，将表单数据按照UTF-8的编码放入缓冲区，在取出来时，自然就已经不会乱码了。在get请求时较为繁琐。需要首先将表单数据以ISO-8859-1字符集转成字节数组，因为request缓冲区的默认字符集是ISO-8859-1,所以以该字符集来将乱码数据转换成字节数组，不会丢失字节，从而保证了表单数据的完整性，进而以UTF-8字符集构造成新的字符串，将中文读出。<br>    但是如果需要将中文数据在页面展示，还需要：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">response.setContentType(&quot;text/html;charset=UTF-8&quot;);//向浏览器指明要以UTF-8字符集解析返回的数据</span><br></pre></td></tr></table></figure>

<p><em><strong>以上是针对单个网页的中文乱码解决方法</strong></em></p>
<span id="more"></span>

<h3 id="在一个web项目中，中文乱码的解决方案"><a href="#在一个web项目中，中文乱码的解决方案" class="headerlink" title="在一个web项目中，中文乱码的解决方案"></a>在一个web项目中，中文乱码的解决方案</h3><h4 id="我认为最佳解决方案就是使用过滤器，拦截所有包含有中文的请求数据，转码后再传递给请求的目标地址。"><a href="#我认为最佳解决方案就是使用过滤器，拦截所有包含有中文的请求数据，转码后再传递给请求的目标地址。" class="headerlink" title="我认为最佳解决方案就是使用过滤器，拦截所有包含有中文的请求数据，转码后再传递给请求的目标地址。"></a>我认为最佳解决方案就是使用过滤器，拦截所有包含有中文的请求数据，转码后再传递给请求的目标地址。</h4><p>依旧以上述表单数据的中文乱码为例：<br>Filter代码如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">public class GenericEncodingFilter implements Filter &#123;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void destroy() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void doFilter(ServletRequest request, ServletResponse response,</span><br><span class="line">            FilterChain chain) throws IOException, ServletException &#123;</span><br><span class="line">        // 转型为与协议相关对象</span><br><span class="line">        HttpServletRequest httpServletRequest = (HttpServletRequest) request;</span><br><span class="line">        // 对request包装增强</span><br><span class="line">        HttpServletRequest myrequest = new MyRequest(httpServletRequest);</span><br><span class="line">        chain.doFilter(myrequest, response);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void init(FilterConfig filterConfig) throws ServletException &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述代码中，我们思考到，之所以在获取表单数据时，拿到的中文都是乱码，是因为request对象本身并不具备对中乱码的解决能力，因此，我们想到以装饰者模式对request进行增强。(当然也可以使用动态代理来解决)<br><strong>关于装饰者模式的注意点：</strong></p>
<ol>
<li>增强的类要和被增强的类实现同一个接口 </li>
<li>增强的类需要传入一个被增强类的引用</li>
</ol>
<p>以下是增强HttpServletRequest对象的实现：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">// 自定义request对象</span><br><span class="line">class MyRequest extends HttpServletRequestWrapper &#123;</span><br><span class="line"></span><br><span class="line">    private HttpServletRequest request;</span><br><span class="line"></span><br><span class="line">    private boolean hasEncode;</span><br><span class="line"></span><br><span class="line">    public MyRequest(HttpServletRequest request) &#123;//此处传入了request的引用</span><br><span class="line">        super(request);// super必须写</span><br><span class="line">        this.request = request;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 对需要增强方法 进行覆盖</span><br><span class="line">    @Override</span><br><span class="line">    public Map getParameterMap() &#123;</span><br><span class="line">        // 先获得请求方式</span><br><span class="line">        String method = request.getMethod();</span><br><span class="line">        if (method.equalsIgnoreCase(&quot;post&quot;)) &#123;</span><br><span class="line">            // post请求</span><br><span class="line">            try &#123;</span><br><span class="line">                // 处理post乱码</span><br><span class="line">                request.setCharacterEncoding(&quot;utf-8&quot;);</span><br><span class="line">                return request.getParameterMap();</span><br><span class="line">            &#125; catch (UnsupportedEncodingException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else if (method.equalsIgnoreCase(&quot;get&quot;)) &#123;</span><br><span class="line">            // get请求</span><br><span class="line">            Map&lt;String, String[]&gt; parameterMap = request.getParameterMap();</span><br><span class="line">            if (!hasEncode) &#123; // 确保get手动编码逻辑只运行一次</span><br><span class="line">                for (String parameterName : parameterMap.keySet()) &#123;</span><br><span class="line">                    String[] values = parameterMap.get(parameterName);</span><br><span class="line">                    if (values != null) &#123;</span><br><span class="line">                        for (int i = 0; i &lt; values.length; i++) &#123;</span><br><span class="line">                            try &#123;</span><br><span class="line">                                // 处理get乱码</span><br><span class="line">                                values[i] = new String(values[i]</span><br><span class="line">                                        .getBytes(&quot;ISO-8859-1&quot;), &quot;utf-8&quot;);</span><br><span class="line">                            &#125; catch (UnsupportedEncodingException e) &#123;</span><br><span class="line">                                e.printStackTrace();</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                hasEncode = true;</span><br><span class="line">            &#125;</span><br><span class="line">            return parameterMap;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return super.getParameterMap();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getParameter(String name) &#123;</span><br><span class="line">        Map&lt;String, String[]&gt; parameterMap = getParameterMap();</span><br><span class="line">        String[] values = parameterMap.get(name);</span><br><span class="line">        if (values == null) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">        return values[0]; // 取回参数的第一个值</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String[] getParameterValues(String name) &#123;</span><br><span class="line">        Map&lt;String, String[]&gt; parameterMap = getParameterMap();</span><br><span class="line">        String[] values = parameterMap.get(name);</span><br><span class="line">        return values;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上述方法中，对request进行了增强，根据请求方法的不同，对request所携带的表单数据进行中文乱码的处理。然后将增强后的request向后传递：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">// 对request包装增强</span><br><span class="line">HttpServletRequest myrequest = new MyRequest(httpServletRequest);</span><br><span class="line">chain.doFilter(myrequest, response);//将包装后的myrequest对象作为request对象向后传递</span><br></pre></td></tr></table></figure>

<p>在web.xml中做如下配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;filter&gt;</span><br><span class="line">    &lt;display-name&gt;GenericEncodingFilter&lt;/display-name&gt;</span><br><span class="line">    &lt;filter-name&gt;GenericEncodingFilter&lt;/filter-name&gt;</span><br><span class="line">    &lt;filter-class&gt;com.wfm.web.filter.GenericEncodingFilter&lt;/filter-class&gt;</span><br><span class="line">  &lt;/filter&gt;</span><br><span class="line">  &lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;GenericEncodingFilter&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;</span><br><span class="line">  &lt;/filter-mapping&gt;</span><br></pre></td></tr></table></figure>
<p>上述配置中<code>&lt;url-pattern&gt;&lt;/url-pattern&gt; </code>,决定了该编码过滤器对哪些请求进行过滤，<code>/*</code>意思是指，对访问该web项目的所有请求都进行过滤。</p>
<h2 id="如何对所有的servlet进行请求的过滤？"><a href="#如何对所有的servlet进行请求的过滤？" class="headerlink" title="如何对所有的servlet进行请求的过滤？"></a>如何对所有的servlet进行请求的过滤？</h2><p>以下是一个servlet的配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;servlet&gt;</span><br><span class="line">    &lt;description&gt;&lt;/description&gt;</span><br><span class="line">    &lt;display-name&gt;FormServlet&lt;/display-name&gt;</span><br><span class="line">    &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt;</span><br><span class="line">    &lt;servlet-class&gt;com.wfm.web.servlet.FormServlet&lt;/servlet-class&gt;</span><br><span class="line">  &lt;/servlet&gt;</span><br><span class="line">  &lt;servlet-mapping&gt;</span><br><span class="line">    &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;/FormServlet&lt;/url-pattern&gt;</span><br><span class="line">  &lt;/servlet-mapping&gt;</span><br></pre></td></tr></table></figure>
<p>在上述配置中，请求通过访问<code>/FormServlet</code>路径触发FormServlet，因此，我们可以拦截所有对该路径的请求。但是如果我们要拦截的是指定的几个Servlet怎么办？此时，想到了如下方法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;servlet&gt;</span><br><span class="line">    &lt;description&gt;&lt;/description&gt;</span><br><span class="line">    &lt;display-name&gt;FormServlet&lt;/display-name&gt;</span><br><span class="line">    &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt;</span><br><span class="line">    &lt;servlet-class&gt;com.wfm.web.servlet.FormServlet&lt;/servlet-class&gt;</span><br><span class="line">&lt;/servlet&gt;</span><br><span class="line">&lt;servlet-mapping&gt;</span><br><span class="line">    &lt;servlet-name&gt;FormServlet&lt;/servlet-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;/servlet/FormServlet&lt;/url-pattern&gt;</span><br><span class="line">&lt;/servlet-mapping&gt;</span><br></pre></td></tr></table></figure>
<p>我们可以在<code>FormServlet</code>的请求路径中加上一个虚拟路径，并将所有待拦截的Servlet的路径都添加<code>/servlet</code>即可构造出一个请求的目录，然后将Filter的<code> &lt;url-pattern&gt;/servlet/*&lt;/url-pattern&gt;</code>,这样就实现了对特定几个servlet的请求过滤。</p>
]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>中文乱码</tag>
      </tags>
  </entry>
  <entry>
    <title>搜索引擎的核心基础技术</title>
    <url>/2017/10/22/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E7%9A%84%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF/</url>
    <content><![CDATA[<h1 id="搜索引擎的核心基础技术"><a href="#搜索引擎的核心基础技术" class="headerlink" title="搜索引擎的核心基础技术"></a>搜索引擎的核心基础技术</h1><h3 id="Lucune-技术"><a href="#Lucune-技术" class="headerlink" title="Lucune 技术"></a>Lucune 技术</h3><span id="more"></span>

<ol>
<li><p>用户搜索的过程</p>
<ul>
<li>用户输入关键词</li>
</ul>
<ul>
<li>——–搜索过程——–</li>
<li>展示结果</li>
</ul>
</li>
<li><p>搜索相关技术</p>
<ul>
<li>lucene是什么？<ul>
<li>lucene是一个用来构建搜索引擎的类库，并不是一个完整的搜索引擎。</li>
<li>lucene核心功能：创建索引和查询索引</li>
</ul>
</li>
<li>solr是什么？<ul>
<li>是一个完整的搜索引擎，是基于lucene。</li>
<li><a href="http://lucene.apache.org/">官网地址</a></li>
</ul>
</li>
</ul>
</li>
<li><p>搜索的过程</p>
</li>
</ol>
<ul>
<li>在mysql数据库进行数据查询–mysql<ul>
<li>去哪里查？  mysql</li>
<li>怎么查询快一点？ select * from 表 where title like “%关键词%”；</li>
<li><strong>在海量数据下放弃数据库的查询技术</strong></li>
</ul>
</li>
<li>lucene的快速查询<ul>
<li>能够通过lucene的索引库实现海量数据的快速查询。</li>
<li>核心功能：创建索引、查询索引<ul>
<li>创建索引时，需要将数据读取之后进行分词。<strong>分词技术</strong></li>
</ul>
</li>
<li>lucene使用一个技术，叫做倒排索引，能够加快查询。<ul>
<li>什么是倒排索引</li>
<li>顺序查找的过程是 如果有一个万个文档，依次打开每个文档，然后逐行查询。</li>
<li>倒排索引时，<strong>通过提前计算，将一个关键词出现在哪些文档中，记录下来成为索引数据</strong>，查询的时候，去索引数据中查找。</li>
<li>举例</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">document1</span><br><span class="line">我爱我的祖国，我的祖国很强大。</span><br><span class="line">document2</span><br><span class="line">我爱你祖国，祖国我爱你。</span><br><span class="line">document3</span><br><span class="line">爱我中华，为中华之崛起而编写爬虫。</span><br></pre></td></tr></table></figure>
<p>需求1：查找出现过“祖国”两个子的文档，并显示出来。<br>顺序查找：<br>readfile1-&gt;readline-&gt;find keyword<br>readfile2-&gt;readline-&gt;find keyword<br>……<br>倒排索引：提前创建索引<br>使用<strong>分词技术</strong>，将读取的一行句子分词。	</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">我|爱|我的|祖国|我的|祖国|很|强大</span><br><span class="line">我爱你|祖国|祖国|我爱你</span><br><span class="line">爱|我|中华|为|中华|之|崛起|而|编写|爬虫</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">我：&#123;document1[1:1],document3[1:2]&#125;</span><br><span class="line">爱：&#123;docuemnt1[1:2],document3[1:1]&#125;</span><br><span class="line">我的：&#123;document1[1:3,1:5]&#125;</span><br><span class="line">祖国：&#123;document1[1:4,1:6],document2[1:2,1:3]&#125;</span><br></pre></td></tr></table></figure>
<p>使用倒排技术进行查询<br>1，先通过关键词找到对应的索引数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">祖国：&#123;document1[1:4,1:6],document2[1:2,1:3]&#125;</span><br></pre></td></tr></table></figure>
<p>2,根据索引数据拼装response返回结果集<br>3，展示给用户结果<br><img src="/img/2017-09-09_093931.png"></p>
<h1 id="4、lucene快速入门"><a href="#4、lucene快速入门" class="headerlink" title="4、lucene快速入门"></a>4、lucene快速入门</h1><ul>
<li>创建maven项目导入lucene的pom依赖<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">	   &lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;lucene-core&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;!-- 查询索引的一些包 --&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;lucene-queries&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;lucene-test-framework&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;!-- 创建索引时，需要创建倒排索引信息，创建倒排需要分词 --&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line">	&lt;!--  如果用户的输入的关键词是个组合词，或者一个句子：为什么打雷不下雨？ --&gt;</span><br><span class="line">	&lt;!-- 用来解析用户的查询意图 --&gt;</span><br><span class="line">	&lt;dependency&gt;</span><br><span class="line">		&lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;</span><br><span class="line">		&lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt;</span><br><span class="line">		&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">	&lt;/dependency&gt;</span><br><span class="line"> &lt;/dependencies&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="4-1、创建索引的过程"><a href="#4-1、创建索引的过程" class="headerlink" title="4.1、创建索引的过程"></a>4.1、创建索引的过程</h1><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LuceneMain</span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">		<span class="comment">// 需求:使用lucene创建索引</span></span><br><span class="line">		<span class="comment">// 1.使用某种技术</span></span><br><span class="line">		<span class="comment">// document---&gt;分词---&gt;倒排---&gt;写入到索引库----介质(硬盘或者内存)</span></span><br><span class="line"></span><br><span class="line">		<span class="type">Directory</span> <span class="variable">d</span> <span class="operator">=</span> FSDirectory.open(<span class="keyword">new</span> <span class="title class_">File</span>(<span class="string">&quot;index&quot;</span>));</span><br><span class="line">		<span class="type">IndexWriterConfig</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexWriterConfig</span>(Version.LATEST, <span class="keyword">new</span> <span class="title class_">StandardAnalyzer</span>());</span><br><span class="line">		<span class="comment">// 第一步:创建indexWriter，传入两个参数，一个directory(FSDriectory,RAMDriectory);一个是IndexWriterConfig</span></span><br><span class="line">		<span class="comment">// IndexWriterConfig,传入两个参数，一个是版本号，一个分词器(中文分词器在3.1时过期了，推荐使用标准StandardAnalyzer)</span></span><br><span class="line">		<span class="type">IndexWriter</span> <span class="variable">indexWriter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IndexWriter</span>(d, conf);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 第二步:对文件创建索引----虎嗅网站：id,title,author,createTime,content,url</span></span><br><span class="line">		<span class="comment">// 一个文档中，有很多字段，想通过title进行查询，这个字段需要被indexed。</span></span><br><span class="line">		<span class="comment">// 如果一个想创建索引，有两种选择，一种不被分词就是完全匹配，一种是分词。</span></span><br><span class="line">		<span class="type">Document</span> <span class="variable">document</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Document</span>();</span><br><span class="line">		<span class="comment">// StringField 不分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">StringField</span>(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;100010&quot;</span>, Store.YES));</span><br><span class="line">		<span class="comment">// TextField 分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">TextField</span>(<span class="string">&quot;title&quot;</span>, <span class="string">&quot;《跑男》《极限挑战3》相继停播，政策已成为今年综艺的最大风险&quot;</span>, Store.YES));</span><br><span class="line">		<span class="comment">// StringField 不分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">StringField</span>(<span class="string">&quot;author&quot;</span>, <span class="string">&quot;文娱商业观察&quot;</span>, Store.YES));</span><br><span class="line">		<span class="comment">// StringField 不分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">StringField</span>(<span class="string">&quot;createTime&quot;</span>, <span class="string">&quot;100010&quot;</span>, Store.YES));</span><br><span class="line">		<span class="comment">// TextField 分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">TextField</span>(<span class="string">&quot;content&quot;</span>,</span><br><span class="line">				<span class="string">&quot;虽然安全播了三季，但每年都要被停播个那么几周。果不其然，这周日《极限挑战》又要停播，联系前段时间东方卫视《金星秀》《今晚80后脱口秀》宣布停播，这次《极限挑战》的停播是在警示谁？&quot;</span>,</span><br><span class="line">				Store.YES));</span><br><span class="line">		<span class="comment">// StringField 不分词</span></span><br><span class="line">		document.add(<span class="keyword">new</span> <span class="title class_">StringField</span>(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;https://www.huxiu.com/article/213893.html&quot;</span>, Store.YES));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 第三步：使用indexWriter创建索引</span></span><br><span class="line">		indexWriter.addDocument(document);</span><br><span class="line">		<span class="comment">// commit将内存中等待的数据提交到硬盘创建索引。</span></span><br><span class="line">		indexWriter.commit();</span><br><span class="line"></span><br><span class="line">		<span class="comment">// 第四步：关闭indexwriter</span></span><br><span class="line">		indexWriter.close();</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/img/2017-09-09_110528.png"></p>
<h1 id="4-2、使用luke工具查看索引库"><a href="#4-2、使用luke工具查看索引库" class="headerlink" title="4.2、使用luke工具查看索引库"></a>4.2、使用luke工具查看索引库</h1><ul>
<li>分析出lucene索引库会存储两种类型的数据<ul>
<li>保存索引数据</li>
<li>保存原始数据<br>  <img src="/img/2017-09-09_105613.png"><br>  <img src="/img/2017-09-09_110305.png"></li>
</ul>
</li>
</ul>
<h1 id="4-3、查询索引的过程"><a href="#4-3、查询索引的过程" class="headerlink" title="4.3、查询索引的过程"></a>4.3、查询索引的过程</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">private static void searchIndex() throws IOException &#123;</span><br><span class="line">		//需求:使用lucene查询索引</span><br><span class="line">		FSDirectory directory = FSDirectory.open(new File(&quot;index&quot;));</span><br><span class="line">		// 第一步:创建IndexSearcher，IndexSearcher 是基于索引库进行查询 ，需要先读取索引库。</span><br><span class="line">		IndexSearcher indexSearcher = new IndexSearcher(DirectoryReader.open(directory));</span><br><span class="line">		// 第二步：设置用户的关键词</span><br><span class="line">		String keyword = &quot;文娱商业观察&quot;;</span><br><span class="line">		// 第三步：根据词条进行查询</span><br><span class="line">		// 将关键词转换成一个term对象，需要指定关键词要查询字段 new Term(&quot;author&quot;, keyword)</span><br><span class="line">		TopDocs res = indexSearcher.search(new TermQuery(new Term(&quot;author&quot;, keyword)), Integer.MAX_VALUE);</span><br><span class="line">		// 第四步：从topdocs中获取数据</span><br><span class="line">		System.out.println(&quot;当前查询命中多少天数据:&quot;+res.totalHits);</span><br><span class="line">		ScoreDoc[] scoreDocs = res.scoreDocs;</span><br><span class="line">		for (ScoreDoc scoreDoc : scoreDocs) &#123;</span><br><span class="line">			System.out.println(&quot;维护在lucene内部的文档编号:&quot;+scoreDoc.doc);</span><br><span class="line">			System.out.println(&quot;当前文档的得分:&quot;+scoreDoc.score);</span><br><span class="line">			//第五步:获取单篇文章的信息</span><br><span class="line">			Document doc = indexSearcher.doc(scoreDoc.doc);</span><br><span class="line">			System.out.println(&quot;id:   &quot;+doc.get(&quot;id&quot;));</span><br><span class="line">			System.out.println(&quot;title:   &quot;+doc.get(&quot;title&quot;));</span><br><span class="line">			System.out.println(&quot;author:   &quot;+doc.get(&quot;author&quot;));</span><br><span class="line">			System.out.println(&quot;createTime:   &quot;+doc.get(&quot;createTime&quot;));</span><br><span class="line">			System.out.println(&quot;content:   &quot;+doc.get(&quot;content&quot;));</span><br><span class="line">			System.out.println(&quot;url:   &quot;+doc.get(&quot;url&quot;));</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>如何知道一次搜索过程中，哪些文档相关性更强？<ul>
<li>10个文件，都包含关键词 “传智播客”</li>
<li>每篇文章中关键词都会出现？<ul>
<li>出现的次数比较多的，相关性比较大。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>#5、在项目中使用ikanalyzer分词器</p>
<ul>
<li>是什么？ <a href="https://www.oschina.net/question/28_61577">中国林良益 2012版本之后 在阿里</a></li>
<li>maven项目，2012年发布jar，没有发布到maven。<ul>
<li>需要将jar包导入到本地mvn仓库<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mvn install:install-file -Dfile=IKAnalyzer2012FF_u1.jar -DgroupId=org.wltea.ik-analyzer -DartifactId=ik-analyzer -Dversion=4.10.2 -Dpackaging=jar </span><br></pre></td></tr></table></figure>
<img src="/img/2017-09-09_143433.png"></li>
</ul>
</li>
<li>在项目中使用<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;4.10.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li>
<li>使用自己的词库<br><img src="/img/2017-09-09_143559.png"><br><img src="/img/2017-09-09_143637.png"></li>
<li>ik分词器和lucene的关系<ul>
<li>ik是一个独立的分词器<br>  <img src="/img/2017-09-09_152113.png"></li>
<li>由于lucene的查询功能底层使用了倒排索引的技术，而倒排索引需要对内容进行分词，才使用了分词技术。</li>
</ul>
</li>
</ul>
<h1 id="6、lucene的花式查询"><a href="#6、lucene的花式查询" class="headerlink" title="6、lucene的花式查询"></a>6、lucene的花式查询</h1><p><img src="/img/2017-09-09_152529.png"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">// TermQuery query = new TermQuery(new Term(&quot;content&quot;, &quot;战&quot;));</span><br><span class="line"></span><br><span class="line">//// 通过两次置换，能够得到一个词条</span><br><span class="line"> Term term = new Term(&quot;content&quot;,&quot;大据数&quot;);</span><br><span class="line"> FuzzyQuery query = new FuzzyQuery(term);</span><br><span class="line"></span><br><span class="line">//通过前缀查询</span><br><span class="line">PrefixQuery query = new PrefixQuery(new Term(&quot;content&quot;, &quot;大&quot;));</span><br><span class="line"></span><br><span class="line">WildcardQuery query = new WildcardQuery(new Term(&quot;content&quot;,&quot;大*&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BooleanQuery query = new BooleanQuery();</span><br><span class="line"></span><br><span class="line">WildcardQuery wildcardQuery = new WildcardQuery(new Term(&quot;content&quot;, &quot;大*&quot;));</span><br><span class="line">query.add(wildcardQuery, Occur.MUST);</span><br><span class="line"></span><br><span class="line">PrefixQuery prefixQuery = new PrefixQuery(new Term(&quot;content&quot;, &quot;金&quot;));</span><br><span class="line">query.add(prefixQuery, Occur.MUST);</span><br><span class="line"></span><br><span class="line">//重要：在大多数情况下，用户的输入不一定是一个词条，</span><br><span class="line">//所以我们需要对用户的输入进行分词，将输入编程多个词条之后进行查询。</span><br><span class="line">Analyzer analyzer = new IKAnalyzer();</span><br><span class="line">QueryParser queryParser = new QueryParser(&quot;content&quot;, analyzer);</span><br><span class="line">Query query =queryParser.parse(&quot;学习大数据&quot;);</span><br><span class="line"></span><br><span class="line">//重要：有时候业务会提供多个字段供用户选择，店铺，商家，旺旺。</span><br><span class="line">MultiFieldQueryParser multiFieldQueryParser </span><br><span class="line">= new MultiFieldQueryParser(new String[]&#123;&quot;title&quot;,&quot;content&quot;&#125;,new IKAnalyzer());</span><br><span class="line">Query query =multiFieldQueryParser.parse(&quot;学习大数据&quot;);</span><br></pre></td></tr></table></figure>

<h1 id="7、如何使用lucene开发一个搜索引擎"><a href="#7、如何使用lucene开发一个搜索引擎" class="headerlink" title="7、如何使用lucene开发一个搜索引擎"></a>7、如何使用lucene开发一个搜索引擎</h1><ul>
<li>对外开发一个controller，这个controller提供两个方法<ul>
<li>init:读取数据库的数据，创建索引<ul>
<li>———dao————-</li>
<li>mysql jdbc DataSource</li>
<li>jdbctemplate mybatis访问</li>
<li>select * from huxiu_article;</li>
<li>———-service———–</li>
<li>得到数据库查询的结果，并封装到list集合当中</li>
<li>迭代list中的元素，将每个元素封装一个document对象<ul>
<li>调用indexWriter.add(doc)创建索引</li>
<li>调用indexWriter.commit()</li>
</ul>
</li>
<li>关闭indexWriter.close()</li>
</ul>
</li>
<li>query:查询方法，主要接受用户输入的关键字。<ul>
<li>——–controller———</li>
<li>获取数据</li>
<li>————service——–</li>
<li>将用户的关键词封装成一个query对象<ul>
<li>使用QueryParser对用户输入的关键词进行分词</li>
<li>indexSearcher.search(query,integer.max)</li>
</ul>
</li>
<li>得到索引库的返回值，封装成一个list对象。</li>
<li>返回list对象给页面</li>
<li>页面通过jstl等等技术，进行展现。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​		</p>
]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
      </tags>
  </entry>
  <entry>
    <title>网站流量日志数据自定义采集</title>
    <url>/2018/04/12/%E7%BD%91%E7%AB%99%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E9%87%87%E9%9B%86(%E5%9F%8B%E7%82%B9)/</url>
    <content><![CDATA[<h3 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h3><p>首先，用户的行为会触发浏览器对被统计页面的一个HTTP请求。比如，让用户打开<br>一个页面时，触发页面中的埋点代码的执行。</p>
<p><img src="/blob/master/image/%E5%9F%8B%E7%82%B9%E6%97%B6%E5%BA%8F%E5%9B%BE.bmp" alt="A"></p>
<p><strong>埋点</strong>：事先在网页中加入的一小段js代码，这个代码片段一般会动态创建一个<br>script标签，并将src属性指向一个单独的js文件，然后这个js文件会从数据采集服务器端返回，并自动触发(js匿名函数自调用)，<br>这个js往往就是真正的数据收集脚本。数据收集完成后，js会请求一个后端的数据收集脚本，这个脚本一般是一个伪装成图片的动态<br>脚本程序，js会将收集到的数据通过HTTP参数的形式传递给后端脚本。后端脚本解析参数并按指定的格式存储该数据。同时可能会在<br>HTTP响应中给客户端种植一些用于追踪的cookie。</p>
<span id="more"></span>

<p><strong>模拟图片资源的请求，是因为 img 标签的 src  属性有跨域请求的特性</strong></p>
<hr>
<h3 id="设计实现"><a href="#设计实现" class="headerlink" title="设计实现"></a>设计实现</h3><p>&emsp;&emsp;根据原理分析结合Google Analytics，搭建一个自定义日志数据采集系统，可以按照如下步骤进行：</p>
<p><img src="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/image/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%9E%B6%E6%9E%84%E5%9B%BE.bmp"></p>
<p>1、确定收集的字段</p>
<p><img src="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/image/%E9%87%87%E9%9B%86%E5%AD%97%E6%AE%B5%E8%A1%A8.bmp"></p>
<p>2、确定埋点代码</p>
<p>&emsp;&emsp;以谷歌的Google Analytics来说，需要在页面中插入它提供的JavaScript片段，这个片段就被称为<strong>埋点代码</strong>。</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&lt;script type=<span class="string">&quot;text/javascript&quot;</span>&gt;</span><br><span class="line"><span class="keyword">var</span> _maq = _maq || [];</span><br><span class="line">_maq.<span class="title function_">push</span>([<span class="string">&#x27;_setAccount&#x27;</span>, <span class="string">&#x27;UA-XXXXX-X&#x27;</span>]);<span class="comment">//向_maq中添加一条配置</span></span><br><span class="line">(<span class="keyword">function</span>(<span class="params"></span>) &#123;</span><br><span class="line"><span class="keyword">var</span> ma = <span class="variable language_">document</span>.<span class="title function_">createElement</span>(<span class="string">&#x27;script&#x27;</span>); ma.<span class="property">type</span> =</span><br><span class="line"><span class="string">&#x27;text/javascript&#x27;</span>; ma.<span class="property">async</span> = <span class="literal">true</span>;</span><br><span class="line">ma.<span class="property">src</span> = (<span class="string">&#x27;https:&#x27;</span> == <span class="variable language_">document</span>.<span class="property">location</span>.<span class="property">protocol</span> ?</span><br><span class="line"><span class="string">&#x27;https://ssl&#x27;</span> : <span class="string">&#x27;http://www&#x27;</span>) + <span class="string">&#x27;.google-analytics.com/ma.js&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> s = <span class="variable language_">document</span>.<span class="title function_">getElementsByTagName</span>(<span class="string">&#x27;script&#x27;</span>)[<span class="number">0</span>];</span><br><span class="line">s.<span class="property">parentNode</span>.<span class="title function_">insertBefore</span>(ma, s);</span><br><span class="line">&#125;)();</span><br><span class="line">&lt;/script&gt;</span><br></pre></td></tr></table></figure>
<p>其中_maq是全局数组，用于放置配置信息，每条配置的格式为：</p>
<p><code>_maq.push([&#39;Action&#39;, &#39;param1&#39;, &#39;param2&#39;, ...]);</code></p>
<p>&emsp;&emsp;后边的匿名函数的代码主要目的就是引入一个外部的js文件(ma.js),方式是通过document.createElement方法<br>创建一个script并将<code>src</code>属性指向数据采集服务器上的ma.js，最后将这个元素插入到dom树上。<br><strong><code>ma.sync=true</code>的意思是异步调用外部js文件，即不阻塞浏览器的解析，待外部下载完成后异步执行，这个是HTML5新引入的</strong>。</p>
<p>3、前端数据收集脚本</p>
<p>&emsp;&emsp;数据收集脚本(ma.js)被请求后会被执行，一般要做如下几件事：</p>
<pre><code>（1）通过浏览器内置的JavaScript对象收集信息，如页面title(document.title)、上一跳(document.referrer)
用户显示器分辨率(windows.screen)、cookie信息(document.cookie)等信息。

（2）解析_maq数组，收集配置信息。可能会包含用户自定义的事件跟踪、业务数据等。

（3）将上面两步收集的数据按预定义格式解析并拼接(get请求参数)

（4）请求一个后端脚本，将信息放在http.request参数中携带给后端脚本。
</code></pre>
<p> 示例代码：<br> <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">(<span class="keyword">function</span> (<span class="params"></span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> params = &#123;&#125;;</span><br><span class="line">    <span class="comment">//Document 对象数据</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="variable language_">document</span>) &#123;</span><br><span class="line">        params.<span class="property">domain</span> = <span class="variable language_">document</span>.<span class="property">domain</span> || <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        params.<span class="property">url</span> = <span class="variable language_">document</span>.<span class="property">URL</span> || <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        params.<span class="property">title</span> = <span class="variable language_">document</span>.<span class="property">title</span> || <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">        params.<span class="property">referrer</span> = <span class="variable language_">document</span>.<span class="property">referrer</span> || <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//Window 对象数据</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="variable language_">window</span> &amp;&amp; <span class="variable language_">window</span>.<span class="property">screen</span>) &#123;</span><br><span class="line">        params.<span class="property">sh</span> = <span class="variable language_">window</span>.<span class="property">screen</span>.<span class="property">height</span> || <span class="number">0</span>;</span><br><span class="line">        params.<span class="property">sw</span> = <span class="variable language_">window</span>.<span class="property">screen</span>.<span class="property">width</span> || <span class="number">0</span>;</span><br><span class="line">        params.<span class="property">cd</span> = <span class="variable language_">window</span>.<span class="property">screen</span>.<span class="property">colorDepth</span> || <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//navigator 对象数据</span></span><br><span class="line">    <span class="keyword">if</span>(navigator) &#123;</span><br><span class="line">        params.<span class="property">lang</span> = navigator.<span class="property">language</span> || <span class="string">&#x27;&#x27;</span>;</span><br><span class="line">     &#125;</span><br><span class="line"><span class="comment">//解析_maq 配置</span></span><br><span class="line"><span class="keyword">if</span>(_maq) &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">var</span> i <span class="keyword">in</span> _maq) &#123;</span><br><span class="line">        <span class="keyword">switch</span>(_maq[i][<span class="number">0</span>]) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&#x27;_setAccount&#x27;</span>:</span><br><span class="line">        params.<span class="property">account</span> = _maq[i][<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        <span class="attr">default</span>:</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//拼接参数串</span></span><br><span class="line"><span class="keyword">var</span> args = <span class="string">&#x27;&#x27;</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">var</span> i <span class="keyword">in</span> params) &#123;</span><br><span class="line">    <span class="keyword">if</span>(args != <span class="string">&#x27;&#x27;</span>) &#123;</span><br><span class="line">        args += <span class="string">&#x27;&amp;&#x27;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    args += i + <span class="string">&#x27;=&#x27;</span> + <span class="built_in">encodeURIComponent</span>(params[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//通过 Image 对象请求后端脚本</span></span><br><span class="line"><span class="keyword">var</span> img = <span class="keyword">new</span> <span class="title class_">Image</span>(<span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">img.<span class="property">src</span> = <span class="string">&#x27;http://xxx.xxxxx.xxxxx/log.gif?&#x27;</span> + args;</span><br><span class="line">&#125;)();</span><br></pre></td></tr></table></figure><br><strong>脚本放在匿名函数里，确保不会污染全局环境。其中log.gif就是后端脚本。</strong></p>
<p>4、后端脚本</p>
<p>&emsp;&emsp;log.gif是一个伪装成GIF图片的脚本。后端脚本一般需要完成以下几件事情：</p>
<p>  （1）解析HTTP请求参数得到信息。</p>
<p>  （2）从web服务器中获取一些客户端无法获取的信息。</p>
<p>  （3）将信息按格式写入log。</p>
<p>  （4）生成一个1*1的空GIF图片作为响应内容并将响应头的Content-type设为image&#x2F;gif。</p>
<p>  （5）在响应头中通过Set-cookie设置一些需要的cookie信息。(用于追踪唯一访客)</p>
<p>基于nginx的日志收集受制于nginx配置本身的逻辑表达能力有限,所以选用OpenResty来做。</p>
<p>&emsp;&emsp;OpenResty是一个基于nginx扩展出的高性能应用开发平台，内部集成了诸多有用的模块，<br>其中的核心模块就是通过ngx_lua模块集成了Lua，从而在nginx配置文件中可以通过Lua，来表述业务。</p>
<p>&emsp;&emsp;Lua是一中轻量小巧使用C语言编写的脚本语言。其设计目的就是为了嵌入应用程序中，<br>从而为应用程序提供灵活的扩展和定制功能。</p>
<p>&emsp;&emsp;首先需要在nginx的配置文件中定义日志格式：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log_format tick</span><br><span class="line">&quot;$msec||$remote_addr||$status||$body_bytes_sent||$u_domain||$u_url|</span><br><span class="line">|$u_title||$u_referrer||$u_sh||$u_sw||$u_cd||$u_lang||$http_user_ag</span><br><span class="line">ent||$u_account&quot;;</span><br></pre></td></tr></table></figure>
<p>这里以 u_开头的是我们待会会自己定义的变量，其它的是 nginx 内置变<br>量。 然后是核心的两个 location：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">location / log.gif &#123;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">伪装成 gif 文件</span></span><br><span class="line">default_type image/gif;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">本身关闭 access_log，通过 subrequest 记录 <span class="built_in">log</span></span></span><br><span class="line">access_log off;</span><br><span class="line">access_by_lua &quot;</span><br><span class="line">-- 用户跟踪 cookie 名为__utrace</span><br><span class="line">local uid = ngx.var.cookie___utrace</span><br><span class="line">if not uid then</span><br><span class="line">-- 如果没有则生成一个跟踪 cookie，算法为</span><br><span class="line">md5(时间戳+IP+客户端信息)</span><br><span class="line">uid = ngx.md5(ngx.now() ..</span><br><span class="line">ngx.var.remote_addr .. ngx.var.http_user_agent)</span><br><span class="line">end</span><br><span class="line">ngx.header[&#x27;Set-Cookie&#x27;] = &#123;&#x27;__utrace=&#x27; .. uid ..</span><br><span class="line">&#x27;; path=/&#x27;&#125;</span><br><span class="line">if ngx.var.arg_domain then</span><br><span class="line">-- 通过 subrequest 子请求到/i-log 记录日志，</span><br><span class="line">将参数和用户跟踪 cookie 带过去</span><br><span class="line">ngx.location.capture(&#x27;/i-log?&#x27; ..</span><br><span class="line">ngx.var.args .. &#x27;&amp;utrace=&#x27; .. uid)</span><br><span class="line">end</span><br><span class="line">&quot;;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">此请求资源本地不缓存</span></span><br><span class="line">add_header Expires &quot;Fri, 01 Jan 1980 00:00:00 GMT&quot;;</span><br><span class="line">add_header Pragma &quot;no-cache&quot;;</span><br><span class="line">add_header Cache-Control &quot;no-cache, max-age=0, mustrevalidate&quot;;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">返回一个 1×1 的空 gif 图片</span></span><br><span class="line">empty_gif;</span><br><span class="line">&#125;</span><br><span class="line">location /i-log &#123;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">内部 location，不允许外部直接访问</span></span><br><span class="line">internal;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置变量，注意需要 unescape，来自 ngx_set_misc 模块</span></span><br><span class="line">set_unescape_uri $u_domain $arg_domain;</span><br><span class="line">set_unescape_uri $u_url $arg_url;</span><br><span class="line">set_unescape_uri $u_title $arg_title;</span><br><span class="line">set_unescape_uri $u_referrer $arg_referrer;</span><br><span class="line">set_unescape_uri $u_sh $arg_sh;</span><br><span class="line">set_unescape_uri $u_sw $arg_sw;</span><br><span class="line">set_unescape_uri $u_cd $arg_cd;</span><br><span class="line">set_unescape_uri $u_lang $arg_lang;</span><br><span class="line">set_unescape_uri $u_account $arg_account;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">打开日志</span></span><br><span class="line">log_subrequest on;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">记录日志到 ma.log 格式为 tick</span></span><br><span class="line">access_log /path/to/logs/directory/ma.log tick;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">输出空字符串</span></span><br><span class="line">echo &#x27;&#x27;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>5、日志切分</p>
<p>&emsp;&emsp;日志格式主要考虑日志分隔符，一般会有一下几种选择：</p>
<ul>
<li>固定数量的字符</li>
<li>制表符分割</li>
<li>空格分隔符</li>
<li>其他一个或多个字符</li>
<li>特定的开始和结束文本</li>
</ul>
<p>6、日志切分</p>
<p>&emsp;&emsp;日志收集系统访问日志时间一长文件变得很大，而且日志放在一个文件不便<br>于管理。 通常要按时间段将日志切分，例如每天或每小时切分一个日志。通过<br>crontab 定时调用一个 shell 脚本实现，如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">_prefix=&quot;/path/to/nginx&quot;</span><br><span class="line">time=`date +%Y%m%d%H`</span><br><span class="line">mv $&#123;_prefix&#125;/logs/ma.log $&#123;_prefix&#125;/logs/ma/ma-$&#123;time&#125;.log</span><br><span class="line">kill -USR1 `cat $&#123;_prefix&#125;/logs/nginx.pid `</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;USR1 通常被用来告知应用程序重载配置文件, 向服务器发送一个 USR1 信号<br>将导致以下步骤的发生：停止接受新的连接，等待当前连接停止，重新载入配置<br>文件，重新打开日志文件，重启服务器，从而实现相对平滑的不关机的更改。</p>
<p>然后向<code>/etc/crontab</code>里加入一行：</p>
<p><code>59 * * * * root /path/to/directory/rotatelog.sh</code></p>
<p>在每个小时的 59 分启动这个脚本进行日志轮转操作。</p>
<h2 id="自定义采集数据实现"><a href="#自定义采集数据实现" class="headerlink" title="自定义采集数据实现"></a>自定义采集数据实现</h2><p>方案一：基本功能实现</p>
<p>1、创建index.html，添加埋点代码，放入nginx默认目录<code>nginx/html</code>下。</p>
<p>2、在默认目录<code>nginx/html</code>下添加一个数据采集脚本<code>ma.js</code>。</p>
<p>3、修改nginx的配置文件，添加自定义相关业务逻辑。</p>
<p>4、启动nginx,通过浏览器访问nginx。</p>
<p>5、观察自定义日志采集文件是否有对应的内容输出<br>  <code>tail -F logs/user_defined.log</code></p>
<p>方案二：页面点击事件</p>
<p>修改方案一中的数据采集脚本，将脚本内的匿名函数自调用改成由点击事件触发执行。</p>
<p>附件：<br><a href="https://github.com/wangfanming/wangfanming.GitHub.io/blob/master/%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E8%87%AA%E5%AE%9A%E4%B9%89%E9%87%87%E9%9B%86/">相关文件下载</a></p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>埋点</tag>
        <tag>日志采集</tag>
      </tags>
  </entry>
</search>
